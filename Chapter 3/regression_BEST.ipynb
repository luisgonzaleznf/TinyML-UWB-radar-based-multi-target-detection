{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import ast\n",
    "import scipy\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    \n",
    "from os import walk\n",
    "import json\n",
    "\n",
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD 516 DATASET FOR FIRST TRAINING AND VALIDATION\n",
    "dataframe_name = \"dataframe516\"\n",
    "pkl_path = f\"./pickle/{dataframe_name}.pkl\"\n",
    "df = pd.read_pickle(pkl_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINE FUNCTIONS FOR SELECTING PARTS OF THE DATASET REGARDING DIFFERENT CONFIGURATIONS OF OCCUPANTS\n",
    "\n",
    "#select only data that have on seat1 a children or empty\n",
    "def select_only_children_on_seat1(df):\n",
    "    select_df = df.copy()\n",
    "    select_df = select_df[(select_df[\"seat1\"] == \"toddler\") | (select_df[\"seat1\"] == \"baby\") | (select_df[\"seat1\"] == \"none\")]\n",
    "    return select_df\n",
    "\n",
    "#select only data that have one ore more pets alone in the back seats (OR NONE)\n",
    "def select_only_pet(df):\n",
    "    select_df = df.copy()\n",
    "    select_df = select_df[((select_df[\"seat1\"] == \"pet\") & (select_df[\"seat2\"] == \"none\") & (select_df[\"seat3\"] == \"none\")) \n",
    "                          | ((select_df[\"seat1\"] == \"none\") & (select_df[\"seat2\"] == \"pet\") & (select_df[\"seat3\"] == \"none\")) \n",
    "                          | ((select_df[\"seat1\"] == \"none\") & (select_df[\"seat2\"] == \"none\") & (select_df[\"seat3\"] == \"pet\"))\n",
    "                          | ((select_df[\"seat1\"] == \"pet\") & (select_df[\"seat2\"] == \"pet\") & (select_df[\"seat3\"] == \"none\"))\n",
    "                          | ((select_df[\"seat1\"] == \"none\") & (select_df[\"seat2\"] == \"pet\") & (select_df[\"seat3\"] == \"pet\"))\n",
    "                          | ((select_df[\"seat1\"] == \"pet\") & (select_df[\"seat2\"] == \"none\") & (select_df[\"seat3\"] == \"pet\"))\n",
    "                          | ((select_df[\"seat1\"] == \"pet\") & (select_df[\"seat2\"] == \"pet\") & (select_df[\"seat3\"] == \"pet\"))\n",
    "                          | ((select_df[\"seat1\"] == \"none\") & (select_df[\"seat2\"] == \"none\") & (select_df[\"seat3\"] == \"none\"))]\n",
    "    return select_df\n",
    "\n",
    "#select only data that have only one target, adult or toddler, in the back seats (OR NONE)\n",
    "def select_only_single(df):\n",
    "    select_df = df.copy()\n",
    "    select_df = select_df[((select_df[\"seat1\"] == \"adult\") & (select_df[\"seat2\"] == \"none\") & (select_df[\"seat3\"] == \"none\")) \n",
    "                          | ((select_df[\"seat1\"] == \"none\") & (select_df[\"seat2\"] == \"adult\") & (select_df[\"seat3\"] == \"none\")) \n",
    "                          | ((select_df[\"seat1\"] == \"none\") & (select_df[\"seat3\"] == \"adult\") & (select_df[\"seat2\"] == \"none\"))\n",
    "                          | ((select_df[\"seat1\"] == \"toddler\") & (select_df[\"seat2\"] == \"none\") & (select_df[\"seat3\"] == \"none\")) \n",
    "                          | ((select_df[\"seat1\"] == \"none\") & (select_df[\"seat2\"] == \"toddler\") & (select_df[\"seat3\"] == \"none\")) \n",
    "                          | ((select_df[\"seat1\"] == \"none\") & (select_df[\"seat3\"] == \"toddler\") & (select_df[\"seat2\"] == \"none\"))\n",
    "                          | ((select_df[\"seat1\"] == \"none\") & (select_df[\"seat3\"] == \"none\") & (select_df[\"seat2\"] == \"none\"))]\n",
    "    return select_df\n",
    "\n",
    "#select only data that have only one target, adult or toddler, in the back seats. (NO NONE) (LUIS)\n",
    "def select_only_single_true(df):\n",
    "    select_df = df.copy()\n",
    "    select_df = select_df[((select_df[\"seat1\"] == \"adult\") & (select_df[\"seat2\"] == \"none\") & (select_df[\"seat3\"] == \"none\")) \n",
    "                          | ((select_df[\"seat1\"] == \"none\") & (select_df[\"seat2\"] == \"adult\") & (select_df[\"seat3\"] == \"none\")) \n",
    "                          | ((select_df[\"seat1\"] == \"none\") & (select_df[\"seat3\"] == \"adult\") & (select_df[\"seat2\"] == \"none\"))\n",
    "                          | ((select_df[\"seat1\"] == \"toddler\") & (select_df[\"seat2\"] == \"none\") & (select_df[\"seat3\"] == \"none\")) \n",
    "                          | ((select_df[\"seat1\"] == \"none\") & (select_df[\"seat2\"] == \"toddler\") & (select_df[\"seat3\"] == \"none\")) \n",
    "                          | ((select_df[\"seat1\"] == \"none\") & (select_df[\"seat3\"] == \"toddler\") & (select_df[\"seat2\"] == \"none\"))]\n",
    "    return select_df\n",
    "\n",
    "#select only data that have one ore more adults alone in the back seats (OR NONE) (LUIS)\n",
    "def select_only_adult(df):\n",
    "    select_df = df.copy()\n",
    "    select_df = select_df[((select_df[\"seat1\"] == \"adult\") & (select_df[\"seat2\"] == \"none\") & (select_df[\"seat3\"] == \"none\")) \n",
    "                          | ((select_df[\"seat1\"] == \"none\") & (select_df[\"seat2\"] == \"adult\") & (select_df[\"seat3\"] == \"none\")) \n",
    "                          | ((select_df[\"seat1\"] == \"none\") & (select_df[\"seat2\"] == \"none\") & (select_df[\"seat3\"] == \"adult\"))\n",
    "                          | ((select_df[\"seat1\"] == \"adult\") & (select_df[\"seat2\"] == \"adult\") & (select_df[\"seat3\"] == \"none\"))\n",
    "                          | ((select_df[\"seat1\"] == \"none\") & (select_df[\"seat2\"] == \"adult\") & (select_df[\"seat3\"] == \"adult\"))\n",
    "                          | ((select_df[\"seat1\"] == \"adult\") & (select_df[\"seat2\"] == \"none\") & (select_df[\"seat3\"] == \"adult\"))\n",
    "                          | ((select_df[\"seat1\"] == \"adult\") & (select_df[\"seat2\"] == \"adult\") & (select_df[\"seat3\"] == \"adult\"))\n",
    "                          | ((select_df[\"seat1\"] == \"none\") & (select_df[\"seat2\"] == \"none\") & (select_df[\"seat3\"] == \"none\"))]\n",
    "    return select_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINE FUNCTIONS FOR ASSIGNING OCCUPANTS\n",
    "\n",
    "#assign occupations status of seats\n",
    "def assign_occupations(df):\n",
    "    for seat_number in range(1,4):\n",
    "        occ_seat = []\n",
    "        seat = 'seat' + str(seat_number)\n",
    "        for x in df[seat]:\n",
    "            if x != 'none':\n",
    "                occ_seat.append(1)\n",
    "            else:\n",
    "                occ_seat.append(0)\n",
    "        df['class' + str(seat_number)] = occ_seat\n",
    "\n",
    "#DEFINE PRESENCE AS AT LEAST 1 SEAT OCCUPIED\n",
    "def assign_presence(df):\n",
    "    presences = []\n",
    "    for index, row in df.iterrows():\n",
    "        presence = row['class1'] or row['class2'] or row['class3']\n",
    "        presences.append(presence)\n",
    "    df['presence'] = presences\n",
    "\n",
    "#ASSING NUMBER OF OCCUPANTS (LUIS)\n",
    "def assign_occupants(df):\n",
    "    occupants = []\n",
    "    for index, row in df.iterrows():\n",
    "        count = 0\n",
    "        count = row['class1'] + row['class2'] + row['class3']\n",
    "        occupants.append(count)\n",
    "    df['occupants'] = occupants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>deviceSerial</th>\n",
       "      <th>fWversion</th>\n",
       "      <th>batteryLevel</th>\n",
       "      <th>hWversion</th>\n",
       "      <th>rawData</th>\n",
       "      <th>fftData</th>\n",
       "      <th>SW Version</th>\n",
       "      <th>seat1</th>\n",
       "      <th>seat2</th>\n",
       "      <th>...</th>\n",
       "      <th>vehicle</th>\n",
       "      <th>temperature</th>\n",
       "      <th>accX</th>\n",
       "      <th>accY</th>\n",
       "      <th>accZ</th>\n",
       "      <th>createdAt</th>\n",
       "      <th>class1</th>\n",
       "      <th>class2</th>\n",
       "      <th>class3</th>\n",
       "      <th>occupants</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>090cu2xvt8o7Gmx9sMFx</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100</td>\n",
       "      <td>1.2</td>\n",
       "      <td>{\"real\":[[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...</td>\n",
       "      <td>[[-105.87874280268608, -102.26725079789395, -1...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>...</td>\n",
       "      <td>FYMCwQblEjJrCt5S9KDV</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{'_seconds': 1603708000, '_nanoseconds': 54000...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0bv8HBSz1AHA8Mhp5qLm</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100</td>\n",
       "      <td>1.2</td>\n",
       "      <td>{\"real\":[[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...</td>\n",
       "      <td>[[-103.23020764266907, -100.18002761432678, -1...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>toddler</td>\n",
       "      <td>adult</td>\n",
       "      <td>...</td>\n",
       "      <td>4GImmlvjC676xXRyFECn</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{'_seconds': 1607352646, '_nanoseconds': 12600...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19xTU1eBTiK4mBieg8T2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100</td>\n",
       "      <td>1.2</td>\n",
       "      <td>{\"real\":[[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...</td>\n",
       "      <td>[[-107.26817809590841, -103.572805055402, -110...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pet</td>\n",
       "      <td>none</td>\n",
       "      <td>...</td>\n",
       "      <td>4GImmlvjC676xXRyFECn</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{'_seconds': 1604935813, '_nanoseconds': 52500...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1eEVvANdi96NtzXSDT1C</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100</td>\n",
       "      <td>1.2</td>\n",
       "      <td>{\"real\":[[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...</td>\n",
       "      <td>[[-108.10489947686006, -105.49311958297311, -1...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>pet</td>\n",
       "      <td>...</td>\n",
       "      <td>4GImmlvjC676xXRyFECn</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{'_seconds': 1604936210, '_nanoseconds': 91100...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2EttR31aXRBWqx8dRzgq</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100</td>\n",
       "      <td>1.2</td>\n",
       "      <td>{\"real\":[[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...</td>\n",
       "      <td>[[-117.84473965177918, -105.55119981094282, -1...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>toddler</td>\n",
       "      <td>baby</td>\n",
       "      <td>...</td>\n",
       "      <td>4GImmlvjC676xXRyFECn</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{'_seconds': 1607771892, '_nanoseconds': 46000...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>z6yKyTEjlNMoFUpBlCxg</td>\n",
       "      <td>0.8.0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{\"real\":[[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...</td>\n",
       "      <td>[[-74.87174084241651, -70.40210124639611, -77....</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>...</td>\n",
       "      <td>emS4JG4Hp2H8d2Ab4ofV</td>\n",
       "      <td>None</td>\n",
       "      <td>-0.936</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>{'_seconds': 1636904824, '_nanoseconds': 43000...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>zBzfzVgNdiH5EHoPWPjz</td>\n",
       "      <td>0.8.0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{\"real\":[[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...</td>\n",
       "      <td>[[-68.41237424396255, -65.80510348327765, -79....</td>\n",
       "      <td>NaN</td>\n",
       "      <td>baby</td>\n",
       "      <td>none</td>\n",
       "      <td>...</td>\n",
       "      <td>6a9YEoNDIb7J4j5yNXVc</td>\n",
       "      <td>None</td>\n",
       "      <td>-0.676</td>\n",
       "      <td>-0.172</td>\n",
       "      <td>-0.58</td>\n",
       "      <td>{'_seconds': 1636898959, '_nanoseconds': 98800...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>zHvZrXdGjZB73NoeNGOT</td>\n",
       "      <td>0.8.0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{\"real\":[[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...</td>\n",
       "      <td>[[-86.55813594069244, -77.12291636247366, -78....</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adult</td>\n",
       "      <td>none</td>\n",
       "      <td>...</td>\n",
       "      <td>X1ILHgwtdueiSTYr8XdK</td>\n",
       "      <td>None</td>\n",
       "      <td>-0.896</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>-0.292</td>\n",
       "      <td>{'_seconds': 1636483781, '_nanoseconds': 19300...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>zII9nav2JaUQCyxzLKy5</td>\n",
       "      <td>0.8.0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{\"real\":[[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...</td>\n",
       "      <td>[[-89.49105064688271, -86.80552730271502, -95....</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>...</td>\n",
       "      <td>HAETRoo2NZj9oKG8JBAB</td>\n",
       "      <td>None</td>\n",
       "      <td>1.06</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.504</td>\n",
       "      <td>{'_seconds': 1620400379, '_nanoseconds': 62000...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>zZNHJm0aBsJjAiNuAVoi</td>\n",
       "      <td>0.8.0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{\"real\":[[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...</td>\n",
       "      <td>[[-98.65451677423928, -91.73755589282831, -102...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>...</td>\n",
       "      <td>4GImmlvjC676xXRyFECn</td>\n",
       "      <td>None</td>\n",
       "      <td>0.648</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.688</td>\n",
       "      <td>{'_seconds': 1624128012, '_nanoseconds': 29500...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>478 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Id deviceSerial fWversion batteryLevel hWversion  \\\n",
       "0    090cu2xvt8o7Gmx9sMFx            2       1.0          100       1.2   \n",
       "1    0bv8HBSz1AHA8Mhp5qLm            2       1.0          100       1.2   \n",
       "2    19xTU1eBTiK4mBieg8T2            2       1.0          100       1.2   \n",
       "3    1eEVvANdi96NtzXSDT1C            2       1.0          100       1.2   \n",
       "4    2EttR31aXRBWqx8dRzgq            2       1.0          100       1.2   \n",
       "..                    ...          ...       ...          ...       ...   \n",
       "473  z6yKyTEjlNMoFUpBlCxg      0.8.0.0      None          100       1.0   \n",
       "474  zBzfzVgNdiH5EHoPWPjz      0.8.0.0      None          100       1.0   \n",
       "475  zHvZrXdGjZB73NoeNGOT      0.8.0.0      None          100       1.0   \n",
       "476  zII9nav2JaUQCyxzLKy5      0.8.0.0      None          100       1.0   \n",
       "477  zZNHJm0aBsJjAiNuAVoi      0.8.0.0         1          100       1.0   \n",
       "\n",
       "                                               rawData  \\\n",
       "0    {\"real\":[[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...   \n",
       "1    {\"real\":[[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...   \n",
       "2    {\"real\":[[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...   \n",
       "3    {\"real\":[[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...   \n",
       "4    {\"real\":[[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...   \n",
       "..                                                 ...   \n",
       "473  {\"real\":[[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...   \n",
       "474  {\"real\":[[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...   \n",
       "475  {\"real\":[[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...   \n",
       "476  {\"real\":[[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...   \n",
       "477  {\"real\":[[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...   \n",
       "\n",
       "                                               fftData SW Version    seat1  \\\n",
       "0    [[-105.87874280268608, -102.26725079789395, -1...        NaN     none   \n",
       "1    [[-103.23020764266907, -100.18002761432678, -1...        NaN  toddler   \n",
       "2    [[-107.26817809590841, -103.572805055402, -110...        NaN      pet   \n",
       "3    [[-108.10489947686006, -105.49311958297311, -1...        NaN     none   \n",
       "4    [[-117.84473965177918, -105.55119981094282, -1...        NaN  toddler   \n",
       "..                                                 ...        ...      ...   \n",
       "473  [[-74.87174084241651, -70.40210124639611, -77....        NaN     none   \n",
       "474  [[-68.41237424396255, -65.80510348327765, -79....        NaN     baby   \n",
       "475  [[-86.55813594069244, -77.12291636247366, -78....        NaN    adult   \n",
       "476  [[-89.49105064688271, -86.80552730271502, -95....        NaN     none   \n",
       "477  [[-98.65451677423928, -91.73755589282831, -102...        NaN     none   \n",
       "\n",
       "     seat2  ...               vehicle temperature   accX   accY   accZ  \\\n",
       "0     none  ...  FYMCwQblEjJrCt5S9KDV        None   None   None   None   \n",
       "1    adult  ...  4GImmlvjC676xXRyFECn        None   None   None   None   \n",
       "2     none  ...  4GImmlvjC676xXRyFECn        None   None   None   None   \n",
       "3      pet  ...  4GImmlvjC676xXRyFECn        None   None   None   None   \n",
       "4     baby  ...  4GImmlvjC676xXRyFECn        None   None   None   None   \n",
       "..     ...  ...                   ...         ...    ...    ...    ...   \n",
       "473   none  ...  emS4JG4Hp2H8d2Ab4ofV        None -0.936 -0.072 -0.156   \n",
       "474   none  ...  6a9YEoNDIb7J4j5yNXVc        None -0.676 -0.172  -0.58   \n",
       "475   none  ...  X1ILHgwtdueiSTYr8XdK        None -0.896 -0.156 -0.292   \n",
       "476   none  ...  HAETRoo2NZj9oKG8JBAB        None   1.06  0.132  0.504   \n",
       "477   none  ...  4GImmlvjC676xXRyFECn        None  0.648  0.008 -0.688   \n",
       "\n",
       "                                             createdAt class1  class2  class3  \\\n",
       "0    {'_seconds': 1603708000, '_nanoseconds': 54000...      0       0       1   \n",
       "1    {'_seconds': 1607352646, '_nanoseconds': 12600...      1       1       1   \n",
       "2    {'_seconds': 1604935813, '_nanoseconds': 52500...      1       0       1   \n",
       "3    {'_seconds': 1604936210, '_nanoseconds': 91100...      0       1       0   \n",
       "4    {'_seconds': 1607771892, '_nanoseconds': 46000...      1       1       0   \n",
       "..                                                 ...    ...     ...     ...   \n",
       "473  {'_seconds': 1636904824, '_nanoseconds': 43000...      0       0       1   \n",
       "474  {'_seconds': 1636898959, '_nanoseconds': 98800...      1       0       0   \n",
       "475  {'_seconds': 1636483781, '_nanoseconds': 19300...      1       0       0   \n",
       "476  {'_seconds': 1620400379, '_nanoseconds': 62000...      0       0       0   \n",
       "477  {'_seconds': 1624128012, '_nanoseconds': 29500...      0       0       0   \n",
       "\n",
       "     occupants  \n",
       "0            1  \n",
       "1            3  \n",
       "2            2  \n",
       "3            1  \n",
       "4            2  \n",
       "..         ...  \n",
       "473          1  \n",
       "474          1  \n",
       "475          1  \n",
       "476          0  \n",
       "477          0  \n",
       "\n",
       "[478 rows x 21 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DETERMINE PRESENCE ON OUR DATASET\n",
    "assign_occupations(df)\n",
    "assign_occupants(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>occupants</th>\n",
       "      <th>class1</th>\n",
       "      <th>class2</th>\n",
       "      <th>class3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>478 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     occupants  class1  class2  class3\n",
       "0            1       0       0       1\n",
       "1            3       1       1       1\n",
       "2            2       1       0       1\n",
       "3            1       0       1       0\n",
       "4            2       1       1       0\n",
       "..         ...     ...     ...     ...\n",
       "473          1       0       0       1\n",
       "474          1       1       0       0\n",
       "475          1       1       0       0\n",
       "476          0       0       0       0\n",
       "477          0       0       0       0\n",
       "\n",
       "[478 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_df = df\n",
    "\n",
    "df_info = complete_df[['occupants', 'class1', 'class2', 'class3']]\n",
    "df_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "406\n",
      "72\n"
     ]
    }
   ],
   "source": [
    "#SPLIT THE DATASET IN TRAIN AND TEST\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(complete_df, test_size=0.15, random_state = 42, stratify=complete_df.occupants)\n",
    "#LENGHT OF THE TEST DATASET\n",
    "print(len(train_df))\n",
    "print(len(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-105.31283117083753\n",
      "14.104729991228874\n",
      "(406, 53, 86, 1)\n",
      "0.25862068965517243\n",
      "0.4064039408866995\n",
      "0.22167487684729065\n",
      "0.11330049261083744\n",
      "376    2\n",
      "407    1\n",
      "417    0\n",
      "271    1\n",
      "211    1\n",
      "      ..\n",
      "224    0\n",
      "309    1\n",
      "332    1\n",
      "63     3\n",
      "329    1\n",
      "Name: occupants, Length: 406, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "working_df = train_df\n",
    "#TRAINING WILL BE DONE WITH THE FFTDATA COLUMN\n",
    "train_list = working_df.fftData\n",
    "train_list = np.array(train_list)\n",
    "train_x = []\n",
    "\n",
    "\"\"\" Here is performed the frequency selection part of the preprocessing. \n",
    "    Since the fft spectrum is divided in two spectrum of 128 bits each, for performing frequency selection \n",
    "    we need to select from both the first part and the second one.\n",
    "    only one/fraction of the frequencies are selected.\n",
    "\"\"\"\n",
    "\n",
    "#Select only first third of both images\n",
    "fraction = 3 \n",
    "fraction_data = int(round(128/fraction)) #fraction_data=43 in this case\n",
    "\n",
    "for i in range(len(train_list)):\n",
    "    #print(len(train_list[i]))\n",
    "    #print(len(train_list[i][0]))\n",
    "    \n",
    "    a = np.array(train_list[i])[:, 0 : fraction_data]\n",
    "    b = np.array(train_list[i])[:, 128 : 128 + fraction_data]\n",
    "    c = np.concatenate((a, b), axis=1)\n",
    "    train_x.append(c)\n",
    "train_arr = []\n",
    "for x in range(len(train_x)):\n",
    "    train_arr.append(np.array(train_x[x]))\n",
    "train_list = train_arr \n",
    "\n",
    "\"\"\"\n",
    "zscore normalization part of the preprocessing. correcting the dimension of the network.\n",
    "\"\"\"\n",
    "\n",
    "print(np.mean(train_list))\n",
    "print(np.std(train_list))\n",
    "train_list = scipy.stats.zscore(train_list, axis=None)\n",
    "\n",
    "#max = np.max(train_list)\n",
    "#min = np.min(train_list)\n",
    "#train_list = np.array([[[(x - min) / (max - min) for x in y] for y in z] for z in train_list])\n",
    "\n",
    "\n",
    "train_tensor = tf.convert_to_tensor(train_list)\n",
    "\n",
    "#Third dimension value is 1\n",
    "train_tensor = tf.expand_dims(train_tensor, -1)\n",
    "\n",
    "print(train_tensor.shape)\n",
    "\n",
    "\"\"\"\n",
    "assigning label \n",
    "\"\"\"\n",
    "\n",
    "train_label = working_df[\"occupants\"]\n",
    "\n",
    "#PROPORTIONS OF THE DATASET\n",
    "passengers0 = 0\n",
    "passengers1 = 0\n",
    "passengers2 = 0\n",
    "passengers3 = 0\n",
    "for occupants in working_df[\"occupants\"]:\n",
    "    if occupants == 0:\n",
    "        passengers0+=1\n",
    "    if occupants == 1:\n",
    "        passengers1+=1\n",
    "    if occupants == 2:\n",
    "        passengers2+=1\n",
    "    if occupants == 3:\n",
    "        passengers3+=1\n",
    "balancing0 = passengers0/len(train_df)\n",
    "balancing1 = passengers1/len(train_df)\n",
    "balancing2 = passengers2/len(train_df)\n",
    "balancing3 = passengers3/len(train_df)\n",
    "\n",
    "print(balancing0)\n",
    "print(balancing1)\n",
    "print(balancing2)\n",
    "print(balancing3)\n",
    "\n",
    "#train_label = tf.keras.utils.to_categorical(train_label, 3)\n",
    "\n",
    "\"\"\"Dimensions of the inputs\"\"\"\n",
    "#53*86 images\n",
    "img_h, img_w = 53, fraction_data*2\n",
    "num_classes=3\n",
    "\n",
    "print(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-105.05868403226422\n",
      "15.283135488519568\n",
      "(72, 53, 86, 1)\n",
      "[0 2 0 2 0 1 1 1 3 0 2 0 0 3 1 3 2 1 2 1 2 3 3 2 1 0 1 2 1 1 0 2 1 1 1 0 0\n",
      " 1 1 2 0 1 0 2 2 1 1 1 2 2 1 3 1 2 0 1 1 1 0 3 2 1 0 1 1 0 0 3 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Perform the same preprocessing steps of the training set to the test set too.\n",
    "\"\"\"\n",
    "test_labels = np.array(test_df[\"occupants\"])\n",
    "\n",
    "#test_labels = tf.keras.utils.to_categorical(test_labels, 3)\n",
    "\n",
    "test_list = test_df[\"fftData\"]\n",
    "test_list = np.array(test_list)\n",
    "test_x = []\n",
    "\n",
    "fraction = 3 \n",
    "fraction_data = int(round(128/fraction))\n",
    "\n",
    "for i in range(len(test_list)):\n",
    "    \n",
    "    a = np.array(test_list[i])[:, 0 : fraction_data]\n",
    "    b = np.array(test_list[i])[:, 128 : 128 + fraction_data]\n",
    "    c = np.concatenate((a, b), axis=1)\n",
    "    test_x.append(c)\n",
    "test_arr = []\n",
    "for x in range(len(test_x)):\n",
    "    test_arr.append(np.array(test_x[x]))\n",
    "test_list = test_arr \n",
    "\n",
    "\n",
    "print(np.mean(test_list))\n",
    "print(np.std(test_list))\n",
    "test_list = scipy.stats.zscore(test_list, axis=None)\n",
    "\n",
    "#max = np.max(train_list)\n",
    "#min = np.min(train_list)\n",
    "#train_list = np.array([[[(x - min) / (max - min) for x in y] for y in z] for z in train_list])\n",
    "test_tensor = tf.convert_to_tensor(test_list)\n",
    "test_tensor = tf.expand_dims(test_tensor, -1)\n",
    "print(test_tensor.shape)\n",
    "test_images = test_tensor\n",
    "print(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading pre-trained network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder='4_classes_test_2'\n",
    "model_name='2-7-3-12-0.3-1'\n",
    "fraction=3\n",
    "n_epoch=400\n",
    "\n",
    "mypath=f'models_full_train/experiment_{folder}/{model_name}/model'\n",
    "filenames = next(walk(mypath), (None, None, []))[2]\n",
    "\n",
    "imported_model = load_model(mypath)\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "for layer in imported_model.layers[:-1]: # go through until last layer\n",
    "    model.add(layer)\n",
    "model.add(tf.keras.layers.Dense(units=1, activation='linear'))\n",
    "for layer in model.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(loss='mse', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to run inference on a TFLite model\n",
    "def run_tflite_model(tflite_file, inputs, targets):\n",
    "\n",
    "  # Initialize the interpreter\n",
    "    interpreter = tf.lite.Interpreter(model_path=str(tflite_file))\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    input_details = interpreter.get_input_details()[0]\n",
    "    output_details = interpreter.get_output_details()[0]\n",
    "\n",
    "    predictions = []\n",
    "    test_image = inputs\n",
    "    test_label = targets\n",
    "\n",
    "    # Check if the input type is quantized, then rescale input data to uint8\n",
    "    #print(input_details['dtype'])\n",
    "    if input_details['dtype'] == np.int8:\n",
    "        #print(\"correct\")\n",
    "        input_scale, input_zero_point = input_details[\"quantization\"]\n",
    "        test_image = test_image / input_scale + input_zero_point\n",
    "\n",
    "    test_image = np.expand_dims(test_image, axis=0).astype(input_details[\"dtype\"])\n",
    "    interpreter.set_tensor(input_details[\"index\"], test_image)\n",
    "    interpreter.invoke()\n",
    "    output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
    "    #print(output)\n",
    "    predictions[i] = output.argmax()\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('------------------------------------------------------------------------')\n",
    "print(f'Training regression model {model_name} with transfer learning')\n",
    "experiment = \"regression_test_12\" #¡¡¡¡¡CHANGE FOLDER!!!!!!\n",
    "\n",
    "inputs = np.array(train_tensor)\n",
    "targets = train_label\n",
    "\n",
    "mae_per_fold = []\n",
    "loss_per_fold = []\n",
    "train_mae_per_fold = []\n",
    "\n",
    "acc_per_fold_quant = []\n",
    "train_acc_per_fold_quant = []\n",
    "\n",
    "fold_no = 1\n",
    "Y_pred_list = []\n",
    "Y_true_list = []\n",
    "\n",
    "#--------------------------------STATIC PARAMETERS------------------------------------------\n",
    "\n",
    "# Optimization params\n",
    "# -------------------\n",
    "\n",
    "# Loss\n",
    "loss = 'mean_squared_error'\n",
    "\n",
    "# learning rate\n",
    "lr = 0.3e-4\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "# -------------------\n",
    "\n",
    "# Validation metrics\n",
    "# ------------------\n",
    "\n",
    "metrics = ['mae','accuracy']\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "n_epoch = 100\n",
    "\n",
    "\n",
    "#------------------------------------CALLBACKS----------------------------------------\n",
    "callbacks = []\n",
    "\n",
    "# Early Stopping\n",
    "# --------------\n",
    "early_stop = False\n",
    "if early_stop:\n",
    "    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=100, restore_best_weights=False,)\n",
    "    callbacks.append(es_callback)\n",
    "\n",
    "\n",
    "class_weights={0: balancing0, 1: balancing1, 2: balancing2, 3: balancing3}\n",
    "history = model.fit(inputs, targets,\n",
    "            class_weight=class_weights,\n",
    "            batch_size=batch_size,\n",
    "            epochs=n_epoch,\n",
    "            validation_data=(test_images, test_labels),\n",
    "            callbacks = callbacks,\n",
    "            verbose=1)\n",
    "print(history.history.keys())\n",
    "mae_per_fold.append(history.history['val_mae'])\n",
    "loss_per_fold.append(history.history['val_loss'])\n",
    "train_mae_per_fold.append(history.history['mae'])\n",
    "\n",
    "Y_prediction = model.predict(test_images)\n",
    "Y_prediction_int=[]\n",
    "for item in Y_prediction:\n",
    "    a = round(item[0])\n",
    "    Y_prediction_int.append(a)\n",
    "\n",
    "accuracy = 0.0\n",
    "for i in range(len(Y_prediction_int)):\n",
    "    if Y_prediction_int[i]==test_labels[i]:\n",
    "        accuracy += 1\n",
    "accuracy = accuracy / len(Y_prediction)\n",
    "#-------------------------------SAVE MODEL-----------------------------------------\n",
    "\n",
    "MODELS_DIR = f'models/experiment_{experiment}/fraction_{fraction}/n_epoch_{n_epoch}/{model_name}/'\n",
    "try:\n",
    "    if not os.path.exists(MODELS_DIR):\n",
    "        os.makedirs(MODELS_DIR)\n",
    "except e:\n",
    "    if e.errno != errno.EEXIST:\n",
    "        raise   \n",
    "    # time.sleep might help here\n",
    "    pass\n",
    "\n",
    "MODEL_TF = MODELS_DIR + f'fold_{fold_no}'\n",
    "model.save(MODEL_TF)\n",
    "# Increase fold number\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "#---------------------------SAVE RESULTS TO JSON---------------------------------------\n",
    "row = {'model': model_name,\n",
    "       'train_mae' : np.mean(train_mae_per_fold, axis=0).tolist(),\n",
    "       'valid_mae' : np.mean(mae_per_fold, axis=0).tolist(),\n",
    "       'Y_true' : Y_prediction_int, \n",
    "       'Y_pred' : test_labels.tolist(),\n",
    "       'valid_accuracy' : accuracy,\n",
    "       'train_acc_quant':  np.mean(train_acc_per_fold_quant), \n",
    "       'valid_acc_quant':  np.mean(acc_per_fold_quant)\n",
    "}\n",
    "JSON_DIR = f'json_child/experiment_{experiment}/fraction_{fraction}/n_epoch_{n_epoch}'\n",
    "if not os.path.exists(JSON_DIR):\n",
    "    os.makedirs(JSON_DIR)\n",
    "try:\n",
    "    with open(f'{JSON_DIR}/{model_name}.json', 'w') as f:\n",
    "        json.dump(row, f)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "\n",
    "#--------------------------PLOT ACCURACIES CURVES ------------------------------\n",
    "plt.clf()\n",
    "plt.plot(np.mean(train_mae_per_fold, axis=0))\n",
    "plt.plot(np.mean(mae_per_fold, axis=0))\n",
    "plt.savefig(f'{JSON_DIR}/{model_name}.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 0, 2, 0, 1, 1, 1, 2, 0, 2, 1, 0, 2, 1, 1, 2, 0, 1, 1, 1, 2, 2, 2, 1, 0, 1, 2, 0, 1, 1, 2, 1, 1, 1, 0, 0, 1, 2, 1, 0, 1, 0, 2, 2, 1, 1, 1, 2, 2, 1, 2, 1, 1, 0, 1, 1, 1, 0, 2, 2, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1]\n",
      "[0, 2, 0, 2, 0, 1, 1, 1, 3, 0, 2, 0, 0, 3, 1, 3, 2, 1, 2, 1, 2, 3, 3, 2, 1, 0, 1, 2, 1, 1, 0, 2, 1, 1, 1, 0, 0, 1, 1, 2, 0, 1, 0, 2, 2, 1, 1, 1, 2, 2, 1, 3, 1, 2, 0, 1, 1, 1, 0, 3, 2, 1, 0, 1, 1, 0, 0, 3, 0, 0, 1, 1]\n",
      "accuracy = 0.7361111111111112\n",
      "[[16  3  0  0]\n",
      " [ 2 26  1  0]\n",
      " [ 0  5 11  0]\n",
      " [ 0  2  6  0]]\n"
     ]
    }
   ],
   "source": [
    "Y_prediction_int=[]\n",
    "for item in Y_prediction:\n",
    "    a = round(item[0])\n",
    "    Y_prediction_int.append(a)\n",
    "print(Y_prediction_int)\n",
    "print(test_labels.tolist())\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "conf = confusion_matrix(test_labels, Y_prediction_int)\n",
    "accuracy = 0.0\n",
    "for i in range(len(Y_prediction_int)):\n",
    "    if Y_prediction_int[i]==test_labels[i]:\n",
    "        accuracy += 1\n",
    "accuracy = accuracy / len(Y_prediction)\n",
    "print(f\"accuracy = {accuracy}\")\n",
    "print(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 0, 2, 0, 1, 1, 1, 2, 0, 2, 1, 0, 2, 1, 1, 2, 0, 1, 1, 1, 2, 2, 2, 1, 0, 1, 2, 0, 1, 1, 2, 1, 1, 1, 0, 0, 1, 2, 1, 0, 1, 0, 2, 2, 1, 1, 1, 2, 2, 1, 2, 1, 1, 0, 1, 1, 1, 0, 2, 2, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1]\n",
      "[0, 2, 0, 2, 0, 1, 1, 1, 2, 0, 2, 0, 0, 2, 1, 2, 2, 1, 2, 1, 2, 2, 2, 2, 1, 0, 1, 2, 1, 1, 0, 2, 1, 1, 1, 0, 0, 1, 1, 2, 0, 1, 0, 2, 2, 1, 1, 1, 2, 2, 1, 2, 1, 2, 0, 1, 1, 1, 0, 2, 2, 1, 0, 1, 1, 0, 0, 2, 0, 0, 1, 1]\n",
      "accuracy = 0.8194444444444444\n",
      "[[16  3  0]\n",
      " [ 2 26  1]\n",
      " [ 0  7 17]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(test_labels)):\n",
    "    if test_labels[i] == 3:\n",
    "        test_labels[i] = 2\n",
    "for i in range(len(Y_prediction_int)):\n",
    "    if Y_prediction_int[i] == 3:\n",
    "        Y_prediction_int[i] = 2\n",
    "            \n",
    "print(Y_prediction_int)\n",
    "print(test_labels.tolist())\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "conf = confusion_matrix(test_labels, Y_prediction_int)\n",
    "accuracy = 0.0\n",
    "for i in range(len(Y_prediction_int)):\n",
    "    if Y_prediction_int[i]==test_labels[i]:\n",
    "        accuracy += 1\n",
    "accuracy = accuracy / len(Y_prediction)\n",
    "print(f\"accuracy = {accuracy}\")\n",
    "print(conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with the full dataset (n_epoch 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies4=[]\n",
    "accuracies3=[]\n",
    "def train_random_states(amount):\n",
    "    for i in range(amount):\n",
    "        print('----------------------------------------------------------------------------')\n",
    "        print(f'RND STATE NUMBER = {i}')\n",
    "        print('----------------------------------------------------------------------------')\n",
    "        \"\"\"SPLIT THE DATASET\"\"\"\n",
    "        from sklearn.model_selection import train_test_split\n",
    "\n",
    "        train_df, test_df = train_test_split(df, test_size=0.15, random_state = i)\n",
    "        \n",
    "        \"\"\"DATA PREPROCESSING\"\"\"\n",
    "        working_df = train_df\n",
    "        #TRAINING WILL BE DONE WITH THE FFTDATA COLUMN\n",
    "        train_list = working_df.fftData\n",
    "        train_list = np.array(train_list)\n",
    "        train_x = []\n",
    "\n",
    "        \"\"\" Here is performed the frequency selection part of the preprocessing. \n",
    "            Since the fft spectrum is divided in two spectrum of 128 bits each, for performing frequency selection \n",
    "            we need to select from both the first part and the second one.\n",
    "            only one/fraction of the frequencies are selected.\n",
    "        \"\"\"\n",
    "\n",
    "        #Select only first third of both images\n",
    "        fraction = 3 \n",
    "        fraction_data = int(round(128/fraction)) #fraction_data=43 in this case\n",
    "\n",
    "        for i in range(len(train_list)):\n",
    "            #print(len(train_list[i]))\n",
    "            #print(len(train_list[i][0]))\n",
    "\n",
    "            a = np.array(train_list[i])[:, 0 : fraction_data]\n",
    "            b = np.array(train_list[i])[:, 128 : 128 + fraction_data]\n",
    "            c = np.concatenate((a, b), axis=1)\n",
    "            train_x.append(c)\n",
    "        train_arr = []\n",
    "        for x in range(len(train_x)):\n",
    "            train_arr.append(np.array(train_x[x]))\n",
    "        train_list = train_arr \n",
    "\n",
    "        \"\"\"\n",
    "        zscore normalization part of the preprocessing. correcting the dimension of the network.\n",
    "        \"\"\"\n",
    "\n",
    "        print(np.mean(train_list))\n",
    "        print(np.std(train_list))\n",
    "        train_list = scipy.stats.zscore(train_list, axis=None)\n",
    "\n",
    "        #max = np.max(train_list)\n",
    "        #min = np.min(train_list)\n",
    "        #train_list = np.array([[[(x - min) / (max - min) for x in y] for y in z] for z in train_list])\n",
    "\n",
    "\n",
    "        train_tensor = tf.convert_to_tensor(train_list)\n",
    "\n",
    "        #Third dimension value is 1\n",
    "        train_tensor = tf.expand_dims(train_tensor, -1)\n",
    "\n",
    "        print(train_tensor.shape)\n",
    "\n",
    "        \"\"\"\n",
    "        assigning label \n",
    "        \"\"\"\n",
    "\n",
    "        train_label = working_df[\"occupants\"]\n",
    "\n",
    "        #PROPORTIONS OF THE DATASET\n",
    "        passengers0 = 0\n",
    "        passengers1 = 0\n",
    "        passengers2 = 0\n",
    "        passengers3 = 0\n",
    "        for occupants in working_df[\"occupants\"]:\n",
    "            if occupants == 0:\n",
    "                passengers0+=1\n",
    "            if occupants == 1:\n",
    "                passengers1+=1\n",
    "            if occupants == 2:\n",
    "                passengers2+=1\n",
    "            if occupants == 3:\n",
    "                passengers3+=1\n",
    "        balancing0 = passengers0/len(train_df)\n",
    "        balancing1 = passengers1/len(train_df)\n",
    "        balancing2 = passengers2/len(train_df)\n",
    "        balancing3 = passengers3/len(train_df)\n",
    "\n",
    "        print(balancing0)\n",
    "        print(balancing1)\n",
    "        print(balancing2)\n",
    "        print(balancing3)\n",
    "\n",
    "        #train_label = tf.keras.utils.to_categorical(train_label, 3)\n",
    "\n",
    "        \"\"\"Dimensions of the inputs\"\"\"\n",
    "        #53*86 images\n",
    "        img_h, img_w = 53, fraction_data*2\n",
    "        num_classes=3\n",
    "\n",
    "        print(train_label)\n",
    "        \n",
    "        \"\"\"\n",
    "        Perform the same preprocessing steps of the training set to the test set too.\n",
    "        \"\"\"\n",
    "        test_labels = np.array(test_df[\"occupants\"])\n",
    "\n",
    "        #test_labels = tf.keras.utils.to_categorical(test_labels, 3)\n",
    "\n",
    "        test_list = test_df[\"fftData\"]\n",
    "        test_list = np.array(test_list)\n",
    "        test_x = []\n",
    "\n",
    "        fraction = 3 \n",
    "        fraction_data = int(round(128/fraction))\n",
    "\n",
    "        for i in range(len(test_list)):\n",
    "\n",
    "            a = np.array(test_list[i])[:, 0 : fraction_data]\n",
    "            b = np.array(test_list[i])[:, 128 : 128 + fraction_data]\n",
    "            c = np.concatenate((a, b), axis=1)\n",
    "            test_x.append(c)\n",
    "        test_arr = []\n",
    "        for x in range(len(test_x)):\n",
    "            test_arr.append(np.array(test_x[x]))\n",
    "        test_list = test_arr \n",
    "\n",
    "\n",
    "        print(np.mean(test_list))\n",
    "        print(np.std(test_list))\n",
    "        test_list = scipy.stats.zscore(test_list, axis=None)\n",
    "\n",
    "        #max = np.max(train_list)\n",
    "        #min = np.min(train_list)\n",
    "        #train_list = np.array([[[(x - min) / (max - min) for x in y] for y in z] for z in train_list])\n",
    "        test_tensor = tf.convert_to_tensor(test_list)\n",
    "        test_tensor = tf.expand_dims(test_tensor, -1)\n",
    "        print(test_tensor.shape)\n",
    "        test_images = test_tensor\n",
    "        print(test_labels)\n",
    "        \n",
    "        \"\"\"NETWORK DESIGN\"\"\"\n",
    "        folder='4_classes_test_2'\n",
    "        model_name='2-7-3-12-0.3-1'\n",
    "        fraction=3\n",
    "\n",
    "        mypath=f'models_full_train/experiment_{folder}/{model_name}/model'\n",
    "        filenames = next(walk(mypath), (None, None, []))[2]\n",
    "\n",
    "        imported_model = load_model(mypath)\n",
    "        model = tf.keras.Sequential()\n",
    "\n",
    "        for layer in imported_model.layers[:-1]: # go through until last layer\n",
    "            model.add(layer)\n",
    "        model.add(tf.keras.layers.Dense(units=1, activation='linear'))\n",
    "        for layer in model.layers[:-1]:\n",
    "            layer.trainable = False\n",
    "\n",
    "        model.compile(loss='mse', metrics=['mae'])\n",
    "        \n",
    "        print('------------------------------------------------------------------------')\n",
    "        print(f'Training regression model {model_name} with transfer learning')\n",
    "        experiment = f\"regression_test_12_{i}\" #¡¡¡¡¡CHANGE FOLDER!!!!!!\n",
    "\n",
    "        inputs = np.array(train_tensor)\n",
    "        targets = train_label\n",
    "\n",
    "        mae_per_fold = []\n",
    "        loss_per_fold = []\n",
    "        train_mae_per_fold = []\n",
    "\n",
    "        acc_per_fold_quant = []\n",
    "        train_acc_per_fold_quant = []\n",
    "\n",
    "        fold_no = 1\n",
    "        Y_pred_list = []\n",
    "        Y_true_list = []\n",
    "\n",
    "        #--------------------------------STATIC PARAMETERS------------------------------------------\n",
    "\n",
    "        # Optimization params\n",
    "        # -------------------\n",
    "\n",
    "        # Loss\n",
    "        loss = 'mean_squared_error'\n",
    "\n",
    "        # learning rate\n",
    "        lr = 0.3e-4\n",
    "\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "        # -------------------\n",
    "\n",
    "        # Validation metrics\n",
    "        # ------------------\n",
    "\n",
    "        metrics = ['mae','accuracy']\n",
    "\n",
    "        batch_size = 32\n",
    "\n",
    "        n_epoch = 100\n",
    "\n",
    "\n",
    "        #------------------------------------CALLBACKS----------------------------------------\n",
    "        callbacks = []\n",
    "\n",
    "        # Early Stopping\n",
    "        # --------------\n",
    "        early_stop = False\n",
    "        if early_stop:\n",
    "            es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=100, restore_best_weights=False,)\n",
    "            callbacks.append(es_callback)\n",
    "\n",
    "\n",
    "        class_weights={0: balancing0, 1: balancing1, 2: balancing2, 3: balancing3}\n",
    "        history = model.fit(inputs, targets,\n",
    "                    class_weight=class_weights,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=n_epoch,\n",
    "                    validation_data=(test_images, test_labels),\n",
    "                    callbacks = callbacks,\n",
    "                    verbose=False)\n",
    "        print(history.history.keys())\n",
    "        mae_per_fold.append(history.history['val_mae'])\n",
    "        loss_per_fold.append(history.history['val_loss'])\n",
    "        train_mae_per_fold.append(history.history['mae'])\n",
    "\n",
    "        Y_prediction = model.predict(test_images)\n",
    "        Y_prediction_int=[]\n",
    "        for item in Y_prediction:\n",
    "            a = round(item[0])\n",
    "            Y_prediction_int.append(a)\n",
    "\n",
    "        accuracy4 = 0.0\n",
    "        for i in range(len(Y_prediction_int)):\n",
    "            if Y_prediction_int[i]==test_labels[i]:\n",
    "                accuracy4 += 1\n",
    "        accuracy4 = accuracy4 / len(Y_prediction)\n",
    "        \n",
    "        Y_true = test_labels\n",
    "        for i in range(len(Y_true)):\n",
    "            if Y_true[i] == 3:\n",
    "                Y_true[i] = 2\n",
    "        for i in range(len(Y_prediction_int)):\n",
    "            if Y_prediction_int[i] == 3:\n",
    "                Y_prediction_int[i] = 2\n",
    "                \n",
    "        accuracy3 = 0.0\n",
    "        for i in range(len(Y_prediction_int)):\n",
    "            if Y_prediction_int[i]==Y_true[i]:\n",
    "                accuracy3 += 1\n",
    "        accuracy3 = accuracy3 / len(Y_prediction)\n",
    "        print(f\"accuracy4 = {accuracy4}\")\n",
    "        print(f\"accuracy3 = {accuracy3}\")\n",
    "        #-------------------------------SAVE MODEL-----------------------------------------\n",
    "\n",
    "        MODELS_DIR = f'models/experiment_{experiment}/fraction_{fraction}/n_epoch_{n_epoch}/{model_name}/'\n",
    "        try:\n",
    "            if not os.path.exists(MODELS_DIR):\n",
    "                os.makedirs(MODELS_DIR)\n",
    "        except e:\n",
    "            if e.errno != errno.EEXIST:\n",
    "                raise   \n",
    "            # time.sleep might help here\n",
    "            pass\n",
    "\n",
    "        MODEL_TF = MODELS_DIR + f'fold_{fold_no}'\n",
    "        model.save(MODEL_TF)\n",
    "        \n",
    "        accuracies4.append(accuracy4)\n",
    "        accuracies3.append(accuracy3)\n",
    "        \n",
    "    row = {'accuracies4' : accuracies4,\n",
    "          'accuracies3' : accuracies3}\n",
    "    JSON_DIR = f'json_child/experiment_regression_BEST/fraction_{fraction}/n_epoch_{n_epoch}'\n",
    "    if not os.path.exists(JSON_DIR):\n",
    "        os.makedirs(JSON_DIR)\n",
    "    try:\n",
    "        with open(f'{JSON_DIR}/accuracies.json', 'w') as f:\n",
    "            json.dump(row, f)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_random_states(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with the full dataset (n_epoch 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies4=[]\n",
    "accuracies3=[]\n",
    "def train_random_states(amount):\n",
    "    for i in range(amount):\n",
    "        print('----------------------------------------------------------------------------')\n",
    "        print(f'RND STATE NUMBER = {i}')\n",
    "        print('----------------------------------------------------------------------------')\n",
    "        \"\"\"SPLIT THE DATASET\"\"\"\n",
    "        from sklearn.model_selection import train_test_split\n",
    "\n",
    "        train_df, test_df = train_test_split(df, test_size=0.15, random_state = i)\n",
    "        \n",
    "        \"\"\"DATA PREPROCESSING\"\"\"\n",
    "        working_df = train_df\n",
    "        #TRAINING WILL BE DONE WITH THE FFTDATA COLUMN\n",
    "        train_list = working_df.fftData\n",
    "        train_list = np.array(train_list)\n",
    "        train_x = []\n",
    "\n",
    "        \"\"\" Here is performed the frequency selection part of the preprocessing. \n",
    "            Since the fft spectrum is divided in two spectrum of 128 bits each, for performing frequency selection \n",
    "            we need to select from both the first part and the second one.\n",
    "            only one/fraction of the frequencies are selected.\n",
    "        \"\"\"\n",
    "\n",
    "        #Select only first third of both images\n",
    "        fraction = 3 \n",
    "        fraction_data = int(round(128/fraction)) #fraction_data=43 in this case\n",
    "\n",
    "        for i in range(len(train_list)):\n",
    "            #print(len(train_list[i]))\n",
    "            #print(len(train_list[i][0]))\n",
    "\n",
    "            a = np.array(train_list[i])[:, 0 : fraction_data]\n",
    "            b = np.array(train_list[i])[:, 128 : 128 + fraction_data]\n",
    "            c = np.concatenate((a, b), axis=1)\n",
    "            train_x.append(c)\n",
    "        train_arr = []\n",
    "        for x in range(len(train_x)):\n",
    "            train_arr.append(np.array(train_x[x]))\n",
    "        train_list = train_arr \n",
    "\n",
    "        \"\"\"\n",
    "        zscore normalization part of the preprocessing. correcting the dimension of the network.\n",
    "        \"\"\"\n",
    "\n",
    "        print(np.mean(train_list))\n",
    "        print(np.std(train_list))\n",
    "        train_list = scipy.stats.zscore(train_list, axis=None)\n",
    "\n",
    "        #max = np.max(train_list)\n",
    "        #min = np.min(train_list)\n",
    "        #train_list = np.array([[[(x - min) / (max - min) for x in y] for y in z] for z in train_list])\n",
    "\n",
    "\n",
    "        train_tensor = tf.convert_to_tensor(train_list)\n",
    "\n",
    "        #Third dimension value is 1\n",
    "        train_tensor = tf.expand_dims(train_tensor, -1)\n",
    "\n",
    "        print(train_tensor.shape)\n",
    "\n",
    "        \"\"\"\n",
    "        assigning label \n",
    "        \"\"\"\n",
    "\n",
    "        train_label = working_df[\"occupants\"]\n",
    "\n",
    "        #PROPORTIONS OF THE DATASET\n",
    "        passengers0 = 0\n",
    "        passengers1 = 0\n",
    "        passengers2 = 0\n",
    "        passengers3 = 0\n",
    "        for occupants in working_df[\"occupants\"]:\n",
    "            if occupants == 0:\n",
    "                passengers0+=1\n",
    "            if occupants == 1:\n",
    "                passengers1+=1\n",
    "            if occupants == 2:\n",
    "                passengers2+=1\n",
    "            if occupants == 3:\n",
    "                passengers3+=1\n",
    "        balancing0 = passengers0/len(train_df)\n",
    "        balancing1 = passengers1/len(train_df)\n",
    "        balancing2 = passengers2/len(train_df)\n",
    "        balancing3 = passengers3/len(train_df)\n",
    "\n",
    "        print(balancing0)\n",
    "        print(balancing1)\n",
    "        print(balancing2)\n",
    "        print(balancing3)\n",
    "\n",
    "        #train_label = tf.keras.utils.to_categorical(train_label, 3)\n",
    "\n",
    "        \"\"\"Dimensions of the inputs\"\"\"\n",
    "        #53*86 images\n",
    "        img_h, img_w = 53, fraction_data*2\n",
    "        num_classes=3\n",
    "\n",
    "        print(train_label)\n",
    "        \n",
    "        \"\"\"\n",
    "        Perform the same preprocessing steps of the training set to the test set too.\n",
    "        \"\"\"\n",
    "        test_labels = np.array(test_df[\"occupants\"])\n",
    "\n",
    "        #test_labels = tf.keras.utils.to_categorical(test_labels, 3)\n",
    "\n",
    "        test_list = test_df[\"fftData\"]\n",
    "        test_list = np.array(test_list)\n",
    "        test_x = []\n",
    "\n",
    "        fraction = 3 \n",
    "        fraction_data = int(round(128/fraction))\n",
    "\n",
    "        for i in range(len(test_list)):\n",
    "\n",
    "            a = np.array(test_list[i])[:, 0 : fraction_data]\n",
    "            b = np.array(test_list[i])[:, 128 : 128 + fraction_data]\n",
    "            c = np.concatenate((a, b), axis=1)\n",
    "            test_x.append(c)\n",
    "        test_arr = []\n",
    "        for x in range(len(test_x)):\n",
    "            test_arr.append(np.array(test_x[x]))\n",
    "        test_list = test_arr \n",
    "\n",
    "\n",
    "        print(np.mean(test_list))\n",
    "        print(np.std(test_list))\n",
    "        test_list = scipy.stats.zscore(test_list, axis=None)\n",
    "\n",
    "        #max = np.max(train_list)\n",
    "        #min = np.min(train_list)\n",
    "        #train_list = np.array([[[(x - min) / (max - min) for x in y] for y in z] for z in train_list])\n",
    "        test_tensor = tf.convert_to_tensor(test_list)\n",
    "        test_tensor = tf.expand_dims(test_tensor, -1)\n",
    "        print(test_tensor.shape)\n",
    "        test_images = test_tensor\n",
    "        print(test_labels)\n",
    "        \n",
    "        \"\"\"NETWORK DESIGN\"\"\"\n",
    "        folder='4_classes_test_2'\n",
    "        model_name='2-7-3-12-0.3-1'\n",
    "        fraction=3\n",
    "\n",
    "        mypath=f'models_full_train/experiment_{folder}/{model_name}/model'\n",
    "        filenames = next(walk(mypath), (None, None, []))[2]\n",
    "\n",
    "        imported_model = load_model(mypath)\n",
    "        model = tf.keras.Sequential()\n",
    "\n",
    "        for layer in imported_model.layers[:-1]: # go through until last layer\n",
    "            model.add(layer)\n",
    "        model.add(tf.keras.layers.Dense(units=1, activation='linear'))\n",
    "        for layer in model.layers[:-1]:\n",
    "            layer.trainable = False\n",
    "\n",
    "        model.compile(loss='mse', metrics=['mae'])\n",
    "        \n",
    "        print('------------------------------------------------------------------------')\n",
    "        print(f'Training regression model {model_name} with transfer learning')\n",
    "        experiment = f\"regression_test_12_{i}\" #¡¡¡¡¡CHANGE FOLDER!!!!!!\n",
    "\n",
    "        inputs = np.array(train_tensor)\n",
    "        targets = train_label\n",
    "\n",
    "        mae_per_fold = []\n",
    "        loss_per_fold = []\n",
    "        train_mae_per_fold = []\n",
    "\n",
    "        acc_per_fold_quant = []\n",
    "        train_acc_per_fold_quant = []\n",
    "\n",
    "        fold_no = 1\n",
    "        Y_pred_list = []\n",
    "        Y_true_list = []\n",
    "\n",
    "        #--------------------------------STATIC PARAMETERS------------------------------------------\n",
    "\n",
    "        # Optimization params\n",
    "        # -------------------\n",
    "\n",
    "        # Loss\n",
    "        loss = 'mean_squared_error'\n",
    "\n",
    "        # learning rate\n",
    "        lr = 0.3e-4\n",
    "\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "        # -------------------\n",
    "\n",
    "        # Validation metrics\n",
    "        # ------------------\n",
    "\n",
    "        metrics = ['mae','accuracy']\n",
    "\n",
    "        batch_size = 32\n",
    "\n",
    "        n_epoch = 200\n",
    "\n",
    "\n",
    "        #------------------------------------CALLBACKS----------------------------------------\n",
    "        callbacks = []\n",
    "\n",
    "        # Early Stopping\n",
    "        # --------------\n",
    "        early_stop = False\n",
    "        if early_stop:\n",
    "            es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=100, restore_best_weights=False,)\n",
    "            callbacks.append(es_callback)\n",
    "\n",
    "\n",
    "        class_weights={0: balancing0, 1: balancing1, 2: balancing2, 3: balancing3}\n",
    "        history = model.fit(inputs, targets,\n",
    "                    class_weight=class_weights,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=n_epoch,\n",
    "                    validation_data=(test_images, test_labels),\n",
    "                    callbacks = callbacks,\n",
    "                    verbose=False)\n",
    "        print(history.history.keys())\n",
    "        mae_per_fold.append(history.history['val_mae'])\n",
    "        loss_per_fold.append(history.history['val_loss'])\n",
    "        train_mae_per_fold.append(history.history['mae'])\n",
    "\n",
    "        Y_prediction = model.predict(test_images)\n",
    "        Y_prediction_int=[]\n",
    "        for item in Y_prediction:\n",
    "            a = round(item[0])\n",
    "            Y_prediction_int.append(a)\n",
    "\n",
    "        accuracy4 = 0.0\n",
    "        for i in range(len(Y_prediction_int)):\n",
    "            if Y_prediction_int[i]==test_labels[i]:\n",
    "                accuracy4 += 1\n",
    "        accuracy4 = accuracy4 / len(Y_prediction)\n",
    "        \n",
    "        Y_true = test_labels\n",
    "        for i in range(len(Y_true)):\n",
    "            if Y_true[i] == 3:\n",
    "                Y_true[i] = 2\n",
    "        for i in range(len(Y_prediction_int)):\n",
    "            if Y_prediction_int[i] == 3:\n",
    "                Y_prediction_int[i] = 2\n",
    "                \n",
    "        accuracy3 = 0.0\n",
    "        for i in range(len(Y_prediction_int)):\n",
    "            if Y_prediction_int[i]==Y_true[i]:\n",
    "                accuracy3 += 1\n",
    "        accuracy3 = accuracy3 / len(Y_prediction)\n",
    "        print(f\"accuracy4 = {accuracy4}\")\n",
    "        print(f\"accuracy3 = {accuracy3}\")\n",
    "        #-------------------------------SAVE MODEL-----------------------------------------\n",
    "\n",
    "        MODELS_DIR = f'models/experiment_{experiment}/fraction_{fraction}/n_epoch_{n_epoch}/{model_name}/'\n",
    "        try:\n",
    "            if not os.path.exists(MODELS_DIR):\n",
    "                os.makedirs(MODELS_DIR)\n",
    "        except e:\n",
    "            if e.errno != errno.EEXIST:\n",
    "                raise   \n",
    "            # time.sleep might help here\n",
    "            pass\n",
    "\n",
    "        MODEL_TF = MODELS_DIR + f'fold_{fold_no}'\n",
    "        model.save(MODEL_TF)\n",
    "        \n",
    "        accuracies4.append(accuracy4)\n",
    "        accuracies3.append(accuracy3)\n",
    "        \n",
    "    row = {'accuracies4' : accuracies4,\n",
    "          'accuracies3' : accuracies3}\n",
    "    JSON_DIR = f'json_child/experiment_regression_BEST/fraction_{fraction}/n_epoch_{n_epoch}'\n",
    "    if not os.path.exists(JSON_DIR):\n",
    "        os.makedirs(JSON_DIR)\n",
    "    try:\n",
    "        with open(f'{JSON_DIR}/accuracies.json', 'w') as f:\n",
    "            json.dump(row, f)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_random_states(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with the full dataset (n_epoch 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies4=[]\n",
    "accuracies3=[]\n",
    "def train_random_states(amount):\n",
    "    for i in range(amount):\n",
    "        print('----------------------------------------------------------------------------')\n",
    "        print(f'RND STATE NUMBER = {i}')\n",
    "        print('----------------------------------------------------------------------------')\n",
    "        \"\"\"SPLIT THE DATASET\"\"\"\n",
    "        from sklearn.model_selection import train_test_split\n",
    "\n",
    "        train_df, test_df = train_test_split(df, test_size=0.15, random_state = i)\n",
    "        \n",
    "        \"\"\"DATA PREPROCESSING\"\"\"\n",
    "        working_df = train_df\n",
    "        #TRAINING WILL BE DONE WITH THE FFTDATA COLUMN\n",
    "        train_list = working_df.fftData\n",
    "        train_list = np.array(train_list)\n",
    "        train_x = []\n",
    "\n",
    "        \"\"\" Here is performed the frequency selection part of the preprocessing. \n",
    "            Since the fft spectrum is divided in two spectrum of 128 bits each, for performing frequency selection \n",
    "            we need to select from both the first part and the second one.\n",
    "            only one/fraction of the frequencies are selected.\n",
    "        \"\"\"\n",
    "\n",
    "        #Select only first third of both images\n",
    "        fraction = 3 \n",
    "        fraction_data = int(round(128/fraction)) #fraction_data=43 in this case\n",
    "\n",
    "        for i in range(len(train_list)):\n",
    "            #print(len(train_list[i]))\n",
    "            #print(len(train_list[i][0]))\n",
    "\n",
    "            a = np.array(train_list[i])[:, 0 : fraction_data]\n",
    "            b = np.array(train_list[i])[:, 128 : 128 + fraction_data]\n",
    "            c = np.concatenate((a, b), axis=1)\n",
    "            train_x.append(c)\n",
    "        train_arr = []\n",
    "        for x in range(len(train_x)):\n",
    "            train_arr.append(np.array(train_x[x]))\n",
    "        train_list = train_arr \n",
    "\n",
    "        \"\"\"\n",
    "        zscore normalization part of the preprocessing. correcting the dimension of the network.\n",
    "        \"\"\"\n",
    "\n",
    "        print(np.mean(train_list))\n",
    "        print(np.std(train_list))\n",
    "        train_list = scipy.stats.zscore(train_list, axis=None)\n",
    "\n",
    "        #max = np.max(train_list)\n",
    "        #min = np.min(train_list)\n",
    "        #train_list = np.array([[[(x - min) / (max - min) for x in y] for y in z] for z in train_list])\n",
    "\n",
    "\n",
    "        train_tensor = tf.convert_to_tensor(train_list)\n",
    "\n",
    "        #Third dimension value is 1\n",
    "        train_tensor = tf.expand_dims(train_tensor, -1)\n",
    "\n",
    "        print(train_tensor.shape)\n",
    "\n",
    "        \"\"\"\n",
    "        assigning label \n",
    "        \"\"\"\n",
    "\n",
    "        train_label = working_df[\"occupants\"]\n",
    "\n",
    "        #PROPORTIONS OF THE DATASET\n",
    "        passengers0 = 0\n",
    "        passengers1 = 0\n",
    "        passengers2 = 0\n",
    "        passengers3 = 0\n",
    "        for occupants in working_df[\"occupants\"]:\n",
    "            if occupants == 0:\n",
    "                passengers0+=1\n",
    "            if occupants == 1:\n",
    "                passengers1+=1\n",
    "            if occupants == 2:\n",
    "                passengers2+=1\n",
    "            if occupants == 3:\n",
    "                passengers3+=1\n",
    "        balancing0 = passengers0/len(train_df)\n",
    "        balancing1 = passengers1/len(train_df)\n",
    "        balancing2 = passengers2/len(train_df)\n",
    "        balancing3 = passengers3/len(train_df)\n",
    "\n",
    "        print(balancing0)\n",
    "        print(balancing1)\n",
    "        print(balancing2)\n",
    "        print(balancing3)\n",
    "\n",
    "        #train_label = tf.keras.utils.to_categorical(train_label, 3)\n",
    "\n",
    "        \"\"\"Dimensions of the inputs\"\"\"\n",
    "        #53*86 images\n",
    "        img_h, img_w = 53, fraction_data*2\n",
    "        num_classes=3\n",
    "\n",
    "        print(train_label)\n",
    "        \n",
    "        \"\"\"\n",
    "        Perform the same preprocessing steps of the training set to the test set too.\n",
    "        \"\"\"\n",
    "        test_labels = np.array(test_df[\"occupants\"])\n",
    "\n",
    "        #test_labels = tf.keras.utils.to_categorical(test_labels, 3)\n",
    "\n",
    "        test_list = test_df[\"fftData\"]\n",
    "        test_list = np.array(test_list)\n",
    "        test_x = []\n",
    "\n",
    "        fraction = 3 \n",
    "        fraction_data = int(round(128/fraction))\n",
    "\n",
    "        for i in range(len(test_list)):\n",
    "\n",
    "            a = np.array(test_list[i])[:, 0 : fraction_data]\n",
    "            b = np.array(test_list[i])[:, 128 : 128 + fraction_data]\n",
    "            c = np.concatenate((a, b), axis=1)\n",
    "            test_x.append(c)\n",
    "        test_arr = []\n",
    "        for x in range(len(test_x)):\n",
    "            test_arr.append(np.array(test_x[x]))\n",
    "        test_list = test_arr \n",
    "\n",
    "\n",
    "        print(np.mean(test_list))\n",
    "        print(np.std(test_list))\n",
    "        test_list = scipy.stats.zscore(test_list, axis=None)\n",
    "\n",
    "        #max = np.max(train_list)\n",
    "        #min = np.min(train_list)\n",
    "        #train_list = np.array([[[(x - min) / (max - min) for x in y] for y in z] for z in train_list])\n",
    "        test_tensor = tf.convert_to_tensor(test_list)\n",
    "        test_tensor = tf.expand_dims(test_tensor, -1)\n",
    "        print(test_tensor.shape)\n",
    "        test_images = test_tensor\n",
    "        print(test_labels)\n",
    "        \n",
    "        \"\"\"NETWORK DESIGN\"\"\"\n",
    "        folder='4_classes_test_2'\n",
    "        model_name='2-7-3-12-0.3-1'\n",
    "        fraction=3\n",
    "\n",
    "        mypath=f'models_full_train/experiment_{folder}/{model_name}/model'\n",
    "        filenames = next(walk(mypath), (None, None, []))[2]\n",
    "\n",
    "        imported_model = load_model(mypath)\n",
    "        model = tf.keras.Sequential()\n",
    "\n",
    "        for layer in imported_model.layers[:-1]: # go through until last layer\n",
    "            model.add(layer)\n",
    "        model.add(tf.keras.layers.Dense(units=1, activation='linear'))\n",
    "        for layer in model.layers[:-1]:\n",
    "            layer.trainable = False\n",
    "\n",
    "        model.compile(loss='mse', metrics=['mae'])\n",
    "        \n",
    "        print('------------------------------------------------------------------------')\n",
    "        print(f'Training regression model {model_name} with transfer learning')\n",
    "        experiment = f\"regression_test_12_{i}\" #¡¡¡¡¡CHANGE FOLDER!!!!!!\n",
    "\n",
    "        inputs = np.array(train_tensor)\n",
    "        targets = train_label\n",
    "\n",
    "        mae_per_fold = []\n",
    "        loss_per_fold = []\n",
    "        train_mae_per_fold = []\n",
    "\n",
    "        acc_per_fold_quant = []\n",
    "        train_acc_per_fold_quant = []\n",
    "\n",
    "        fold_no = 1\n",
    "        Y_pred_list = []\n",
    "        Y_true_list = []\n",
    "\n",
    "        #--------------------------------STATIC PARAMETERS------------------------------------------\n",
    "\n",
    "        # Optimization params\n",
    "        # -------------------\n",
    "\n",
    "        # Loss\n",
    "        loss = 'mean_squared_error'\n",
    "\n",
    "        # learning rate\n",
    "        lr = 0.3e-4\n",
    "\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "        # -------------------\n",
    "\n",
    "        # Validation metrics\n",
    "        # ------------------\n",
    "\n",
    "        metrics = ['mae','accuracy']\n",
    "\n",
    "        batch_size = 32\n",
    "\n",
    "        n_epoch = 50\n",
    "\n",
    "\n",
    "        #------------------------------------CALLBACKS----------------------------------------\n",
    "        callbacks = []\n",
    "\n",
    "        # Early Stopping\n",
    "        # --------------\n",
    "        early_stop = False\n",
    "        if early_stop:\n",
    "            es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=100, restore_best_weights=False,)\n",
    "            callbacks.append(es_callback)\n",
    "\n",
    "\n",
    "        class_weights={0: balancing0, 1: balancing1, 2: balancing2, 3: balancing3}\n",
    "        history = model.fit(inputs, targets,\n",
    "                    class_weight=class_weights,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=n_epoch,\n",
    "                    validation_data=(test_images, test_labels),\n",
    "                    callbacks = callbacks,\n",
    "                    verbose=False)\n",
    "        print(history.history.keys())\n",
    "        mae_per_fold.append(history.history['val_mae'])\n",
    "        loss_per_fold.append(history.history['val_loss'])\n",
    "        train_mae_per_fold.append(history.history['mae'])\n",
    "\n",
    "        Y_prediction = model.predict(test_images)\n",
    "        Y_prediction_int=[]\n",
    "        for item in Y_prediction:\n",
    "            a = round(item[0])\n",
    "            Y_prediction_int.append(a)\n",
    "\n",
    "        accuracy4 = 0.0\n",
    "        for i in range(len(Y_prediction_int)):\n",
    "            if Y_prediction_int[i]==test_labels[i]:\n",
    "                accuracy4 += 1\n",
    "        accuracy4 = accuracy4 / len(Y_prediction)\n",
    "        \n",
    "        Y_true = test_labels\n",
    "        for i in range(len(Y_true)):\n",
    "            if Y_true[i] == 3:\n",
    "                Y_true[i] = 2\n",
    "        for i in range(len(Y_prediction_int)):\n",
    "            if Y_prediction_int[i] == 3:\n",
    "                Y_prediction_int[i] = 2\n",
    "                \n",
    "        accuracy3 = 0.0\n",
    "        for i in range(len(Y_prediction_int)):\n",
    "            if Y_prediction_int[i]==Y_true[i]:\n",
    "                accuracy3 += 1\n",
    "        accuracy3 = accuracy3 / len(Y_prediction)\n",
    "        print(f\"accuracy4 = {accuracy4}\")\n",
    "        print(f\"accuracy3 = {accuracy3}\")\n",
    "        #-------------------------------SAVE MODEL-----------------------------------------\n",
    "\n",
    "        MODELS_DIR = f'models/experiment_{experiment}/fraction_{fraction}/n_epoch_{n_epoch}/{model_name}/'\n",
    "        try:\n",
    "            if not os.path.exists(MODELS_DIR):\n",
    "                os.makedirs(MODELS_DIR)\n",
    "        except e:\n",
    "            if e.errno != errno.EEXIST:\n",
    "                raise   \n",
    "            # time.sleep might help here\n",
    "            pass\n",
    "\n",
    "        MODEL_TF = MODELS_DIR + f'fold_{fold_no}'\n",
    "        model.save(MODEL_TF)\n",
    "        \n",
    "        accuracies4.append(accuracy4)\n",
    "        accuracies3.append(accuracy3)\n",
    "        \n",
    "    row = {'accuracies4' : accuracies4,\n",
    "          'accuracies3' : accuracies3}\n",
    "    JSON_DIR = f'json_child/experiment_regression_BEST/fraction_{fraction}/n_epoch_{n_epoch}'\n",
    "    if not os.path.exists(JSON_DIR):\n",
    "        os.makedirs(JSON_DIR)\n",
    "    try:\n",
    "        with open(f'{JSON_DIR}/accuracies.json', 'w') as f:\n",
    "            json.dump(row, f)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_random_states(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with the full dataset (n_epoch 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies4=[]\n",
    "accuracies3=[]\n",
    "def train_random_states(amount):\n",
    "    for i in range(amount):\n",
    "        print('----------------------------------------------------------------------------')\n",
    "        print(f'RND STATE NUMBER = {i}')\n",
    "        print('----------------------------------------------------------------------------')\n",
    "        \"\"\"SPLIT THE DATASET\"\"\"\n",
    "        from sklearn.model_selection import train_test_split\n",
    "\n",
    "        train_df, test_df = train_test_split(df, test_size=0.15, random_state = i)\n",
    "        \n",
    "        \"\"\"DATA PREPROCESSING\"\"\"\n",
    "        working_df = train_df\n",
    "        #TRAINING WILL BE DONE WITH THE FFTDATA COLUMN\n",
    "        train_list = working_df.fftData\n",
    "        train_list = np.array(train_list)\n",
    "        train_x = []\n",
    "\n",
    "        \"\"\" Here is performed the frequency selection part of the preprocessing. \n",
    "            Since the fft spectrum is divided in two spectrum of 128 bits each, for performing frequency selection \n",
    "            we need to select from both the first part and the second one.\n",
    "            only one/fraction of the frequencies are selected.\n",
    "        \"\"\"\n",
    "\n",
    "        #Select only first third of both images\n",
    "        fraction = 3 \n",
    "        fraction_data = int(round(128/fraction)) #fraction_data=43 in this case\n",
    "\n",
    "        for i in range(len(train_list)):\n",
    "            #print(len(train_list[i]))\n",
    "            #print(len(train_list[i][0]))\n",
    "\n",
    "            a = np.array(train_list[i])[:, 0 : fraction_data]\n",
    "            b = np.array(train_list[i])[:, 128 : 128 + fraction_data]\n",
    "            c = np.concatenate((a, b), axis=1)\n",
    "            train_x.append(c)\n",
    "        train_arr = []\n",
    "        for x in range(len(train_x)):\n",
    "            train_arr.append(np.array(train_x[x]))\n",
    "        train_list = train_arr \n",
    "\n",
    "        \"\"\"\n",
    "        zscore normalization part of the preprocessing. correcting the dimension of the network.\n",
    "        \"\"\"\n",
    "\n",
    "        print(np.mean(train_list))\n",
    "        print(np.std(train_list))\n",
    "        train_list = scipy.stats.zscore(train_list, axis=None)\n",
    "\n",
    "        #max = np.max(train_list)\n",
    "        #min = np.min(train_list)\n",
    "        #train_list = np.array([[[(x - min) / (max - min) for x in y] for y in z] for z in train_list])\n",
    "\n",
    "\n",
    "        train_tensor = tf.convert_to_tensor(train_list)\n",
    "\n",
    "        #Third dimension value is 1\n",
    "        train_tensor = tf.expand_dims(train_tensor, -1)\n",
    "\n",
    "        print(train_tensor.shape)\n",
    "\n",
    "        \"\"\"\n",
    "        assigning label \n",
    "        \"\"\"\n",
    "\n",
    "        train_label = working_df[\"occupants\"]\n",
    "\n",
    "        #PROPORTIONS OF THE DATASET\n",
    "        passengers0 = 0\n",
    "        passengers1 = 0\n",
    "        passengers2 = 0\n",
    "        passengers3 = 0\n",
    "        for occupants in working_df[\"occupants\"]:\n",
    "            if occupants == 0:\n",
    "                passengers0+=1\n",
    "            if occupants == 1:\n",
    "                passengers1+=1\n",
    "            if occupants == 2:\n",
    "                passengers2+=1\n",
    "            if occupants == 3:\n",
    "                passengers3+=1\n",
    "        balancing0 = passengers0/len(train_df)\n",
    "        balancing1 = passengers1/len(train_df)\n",
    "        balancing2 = passengers2/len(train_df)\n",
    "        balancing3 = passengers3/len(train_df)\n",
    "\n",
    "        print(balancing0)\n",
    "        print(balancing1)\n",
    "        print(balancing2)\n",
    "        print(balancing3)\n",
    "\n",
    "        #train_label = tf.keras.utils.to_categorical(train_label, 3)\n",
    "\n",
    "        \"\"\"Dimensions of the inputs\"\"\"\n",
    "        #53*86 images\n",
    "        img_h, img_w = 53, fraction_data*2\n",
    "        num_classes=3\n",
    "\n",
    "        print(train_label)\n",
    "        \n",
    "        \"\"\"\n",
    "        Perform the same preprocessing steps of the training set to the test set too.\n",
    "        \"\"\"\n",
    "        test_labels = np.array(test_df[\"occupants\"])\n",
    "\n",
    "        #test_labels = tf.keras.utils.to_categorical(test_labels, 3)\n",
    "\n",
    "        test_list = test_df[\"fftData\"]\n",
    "        test_list = np.array(test_list)\n",
    "        test_x = []\n",
    "\n",
    "        fraction = 3 \n",
    "        fraction_data = int(round(128/fraction))\n",
    "\n",
    "        for i in range(len(test_list)):\n",
    "\n",
    "            a = np.array(test_list[i])[:, 0 : fraction_data]\n",
    "            b = np.array(test_list[i])[:, 128 : 128 + fraction_data]\n",
    "            c = np.concatenate((a, b), axis=1)\n",
    "            test_x.append(c)\n",
    "        test_arr = []\n",
    "        for x in range(len(test_x)):\n",
    "            test_arr.append(np.array(test_x[x]))\n",
    "        test_list = test_arr \n",
    "\n",
    "\n",
    "        print(np.mean(test_list))\n",
    "        print(np.std(test_list))\n",
    "        test_list = scipy.stats.zscore(test_list, axis=None)\n",
    "\n",
    "        #max = np.max(train_list)\n",
    "        #min = np.min(train_list)\n",
    "        #train_list = np.array([[[(x - min) / (max - min) for x in y] for y in z] for z in train_list])\n",
    "        test_tensor = tf.convert_to_tensor(test_list)\n",
    "        test_tensor = tf.expand_dims(test_tensor, -1)\n",
    "        print(test_tensor.shape)\n",
    "        test_images = test_tensor\n",
    "        print(test_labels)\n",
    "        \n",
    "        \"\"\"NETWORK DESIGN\"\"\"\n",
    "        folder='4_classes_test_2'\n",
    "        model_name='2-7-3-12-0.3-1'\n",
    "        fraction=3\n",
    "\n",
    "        mypath=f'models_full_train/experiment_{folder}/{model_name}/model'\n",
    "        filenames = next(walk(mypath), (None, None, []))[2]\n",
    "\n",
    "        imported_model = load_model(mypath)\n",
    "        model = tf.keras.Sequential()\n",
    "\n",
    "        for layer in imported_model.layers[:-1]: # go through until last layer\n",
    "            model.add(layer)\n",
    "        model.add(tf.keras.layers.Dense(units=1, activation='linear'))\n",
    "        for layer in model.layers[:-1]:\n",
    "            layer.trainable = False\n",
    "\n",
    "        model.compile(loss='mse', metrics=['mae'])\n",
    "        \n",
    "        print('------------------------------------------------------------------------')\n",
    "        print(f'Training regression model {model_name} with transfer learning')\n",
    "        experiment = f\"regression_test_12_{i}\" #¡¡¡¡¡CHANGE FOLDER!!!!!!\n",
    "\n",
    "        inputs = np.array(train_tensor)\n",
    "        targets = train_label\n",
    "\n",
    "        mae_per_fold = []\n",
    "        loss_per_fold = []\n",
    "        train_mae_per_fold = []\n",
    "\n",
    "        acc_per_fold_quant = []\n",
    "        train_acc_per_fold_quant = []\n",
    "\n",
    "        fold_no = 1\n",
    "        Y_pred_list = []\n",
    "        Y_true_list = []\n",
    "\n",
    "        #--------------------------------STATIC PARAMETERS------------------------------------------\n",
    "\n",
    "        # Optimization params\n",
    "        # -------------------\n",
    "\n",
    "        # Loss\n",
    "        loss = 'mean_squared_error'\n",
    "\n",
    "        # learning rate\n",
    "        lr = 0.3e-4\n",
    "\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "        # -------------------\n",
    "\n",
    "        # Validation metrics\n",
    "        # ------------------\n",
    "\n",
    "        metrics = ['mae','accuracy']\n",
    "\n",
    "        batch_size = 32\n",
    "\n",
    "        n_epoch = 300\n",
    "\n",
    "\n",
    "        #------------------------------------CALLBACKS----------------------------------------\n",
    "        callbacks = []\n",
    "\n",
    "        # Early Stopping\n",
    "        # --------------\n",
    "        early_stop = False\n",
    "        if early_stop:\n",
    "            es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=100, restore_best_weights=False,)\n",
    "            callbacks.append(es_callback)\n",
    "\n",
    "\n",
    "        class_weights={0: balancing0, 1: balancing1, 2: balancing2, 3: balancing3}\n",
    "        history = model.fit(inputs, targets,\n",
    "                    class_weight=class_weights,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=n_epoch,\n",
    "                    validation_data=(test_images, test_labels),\n",
    "                    callbacks = callbacks,\n",
    "                    verbose=False)\n",
    "        print(history.history.keys())\n",
    "        mae_per_fold.append(history.history['val_mae'])\n",
    "        loss_per_fold.append(history.history['val_loss'])\n",
    "        train_mae_per_fold.append(history.history['mae'])\n",
    "\n",
    "        Y_prediction = model.predict(test_images)\n",
    "        Y_prediction_int=[]\n",
    "        for item in Y_prediction:\n",
    "            a = round(item[0])\n",
    "            Y_prediction_int.append(a)\n",
    "\n",
    "        accuracy4 = 0.0\n",
    "        for i in range(len(Y_prediction_int)):\n",
    "            if Y_prediction_int[i]==test_labels[i]:\n",
    "                accuracy4 += 1\n",
    "        accuracy4 = accuracy4 / len(Y_prediction)\n",
    "        \n",
    "        Y_true = test_labels\n",
    "        for i in range(len(Y_true)):\n",
    "            if Y_true[i] == 3:\n",
    "                Y_true[i] = 2\n",
    "        for i in range(len(Y_prediction_int)):\n",
    "            if Y_prediction_int[i] == 3:\n",
    "                Y_prediction_int[i] = 2\n",
    "                \n",
    "        accuracy3 = 0.0\n",
    "        for i in range(len(Y_prediction_int)):\n",
    "            if Y_prediction_int[i]==Y_true[i]:\n",
    "                accuracy3 += 1\n",
    "        accuracy3 = accuracy3 / len(Y_prediction)\n",
    "        print(f\"accuracy4 = {accuracy4}\")\n",
    "        print(f\"accuracy3 = {accuracy3}\")\n",
    "        #-------------------------------SAVE MODEL-----------------------------------------\n",
    "\n",
    "        MODELS_DIR = f'models/experiment_{experiment}/fraction_{fraction}/n_epoch_{n_epoch}/{model_name}/'\n",
    "        try:\n",
    "            if not os.path.exists(MODELS_DIR):\n",
    "                os.makedirs(MODELS_DIR)\n",
    "        except e:\n",
    "            if e.errno != errno.EEXIST:\n",
    "                raise   \n",
    "            # time.sleep might help here\n",
    "            pass\n",
    "\n",
    "        MODEL_TF = MODELS_DIR + f'fold_{fold_no}'\n",
    "        model.save(MODEL_TF)\n",
    "        \n",
    "        accuracies4.append(accuracy4)\n",
    "        accuracies3.append(accuracy3)\n",
    "        \n",
    "    row = {'accuracies4' : accuracies4,\n",
    "          'accuracies3' : accuracies3}\n",
    "    JSON_DIR = f'json_child/experiment_regression_BEST/fraction_{fraction}/n_epoch_{n_epoch}'\n",
    "    if not os.path.exists(JSON_DIR):\n",
    "        os.makedirs(JSON_DIR)\n",
    "    try:\n",
    "        with open(f'{JSON_DIR}/accuracies.json', 'w') as f:\n",
    "            json.dump(row, f)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_random_states(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results are saved on a .json file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
