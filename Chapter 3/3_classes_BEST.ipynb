{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import ast\n",
    "import scipy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD 516 DATASET FOR FIRST TRAINING AND VALIDATION\n",
    "dataframe_name = \"dataframe516\"\n",
    "pkl_path = f\"./pickle/{dataframe_name}.pkl\"\n",
    "df = pd.read_pickle(pkl_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINE FUNCTIONS FOR SELECTING PARTS OF THE DATASET REGARDING DIFFERENT CONFIGURATIONS OF OCCUPANTS\n",
    "\n",
    "#select only data that have on seat1 a children or empty\n",
    "def select_only_children_on_seat1(df):\n",
    "    select_df = df.copy()\n",
    "    select_df = select_df[(select_df[\"seat1\"] == \"toddler\") | (select_df[\"seat1\"] == \"baby\") | (select_df[\"seat1\"] == \"none\")]\n",
    "    return select_df\n",
    "\n",
    "#select only data that have one ore more pets alone in the back seats (OR NONE)\n",
    "def select_only_pet(df):\n",
    "    select_df = df.copy()\n",
    "    select_df = select_df[((select_df[\"seat1\"] == \"pet\") & (select_df[\"seat2\"] == \"none\") & (select_df[\"seat3\"] == \"none\")) \n",
    "                          | ((select_df[\"seat1\"] == \"none\") & (select_df[\"seat2\"] == \"pet\") & (select_df[\"seat3\"] == \"none\")) \n",
    "                          | ((select_df[\"seat1\"] == \"none\") & (select_df[\"seat2\"] == \"none\") & (select_df[\"seat3\"] == \"pet\"))\n",
    "                          | ((select_df[\"seat1\"] == \"pet\") & (select_df[\"seat2\"] == \"pet\") & (select_df[\"seat3\"] == \"none\"))\n",
    "                          | ((select_df[\"seat1\"] == \"none\") & (select_df[\"seat2\"] == \"pet\") & (select_df[\"seat3\"] == \"pet\"))\n",
    "                          | ((select_df[\"seat1\"] == \"pet\") & (select_df[\"seat2\"] == \"none\") & (select_df[\"seat3\"] == \"pet\"))\n",
    "                          | ((select_df[\"seat1\"] == \"pet\") & (select_df[\"seat2\"] == \"pet\") & (select_df[\"seat3\"] == \"pet\"))\n",
    "                          | ((select_df[\"seat1\"] == \"none\") & (select_df[\"seat2\"] == \"none\") & (select_df[\"seat3\"] == \"none\"))]\n",
    "    return select_df\n",
    "\n",
    "#select only data that have only one target, adult or toddler, in the back seats (OR NONE)\n",
    "def select_only_single(df):\n",
    "    select_df = df.copy()\n",
    "    select_df = select_df[((select_df[\"seat1\"] == \"adult\") & (select_df[\"seat2\"] == \"none\") & (select_df[\"seat3\"] == \"none\")) \n",
    "                          | ((select_df[\"seat1\"] == \"none\") & (select_df[\"seat2\"] == \"adult\") & (select_df[\"seat3\"] == \"none\")) \n",
    "                          | ((select_df[\"seat1\"] == \"none\") & (select_df[\"seat3\"] == \"adult\") & (select_df[\"seat2\"] == \"none\"))\n",
    "                          | ((select_df[\"seat1\"] == \"toddler\") & (select_df[\"seat2\"] == \"none\") & (select_df[\"seat3\"] == \"none\")) \n",
    "                          | ((select_df[\"seat1\"] == \"none\") & (select_df[\"seat2\"] == \"toddler\") & (select_df[\"seat3\"] == \"none\")) \n",
    "                          | ((select_df[\"seat1\"] == \"none\") & (select_df[\"seat3\"] == \"toddler\") & (select_df[\"seat2\"] == \"none\"))\n",
    "                          | ((select_df[\"seat1\"] == \"none\") & (select_df[\"seat3\"] == \"none\") & (select_df[\"seat2\"] == \"none\"))]\n",
    "    return select_df\n",
    "\n",
    "#select only data that have only one target, adult or toddler, in the back seats. (NO NONE)\n",
    "def select_only_single_true(df):\n",
    "    select_df = df.copy()\n",
    "    select_df = select_df[((select_df[\"seat1\"] == \"adult\") & (select_df[\"seat2\"] == \"none\") & (select_df[\"seat3\"] == \"none\")) \n",
    "                          | ((select_df[\"seat1\"] == \"none\") & (select_df[\"seat2\"] == \"adult\") & (select_df[\"seat3\"] == \"none\")) \n",
    "                          | ((select_df[\"seat1\"] == \"none\") & (select_df[\"seat3\"] == \"adult\") & (select_df[\"seat2\"] == \"none\"))\n",
    "                          | ((select_df[\"seat1\"] == \"toddler\") & (select_df[\"seat2\"] == \"none\") & (select_df[\"seat3\"] == \"none\")) \n",
    "                          | ((select_df[\"seat1\"] == \"none\") & (select_df[\"seat2\"] == \"toddler\") & (select_df[\"seat3\"] == \"none\")) \n",
    "                          | ((select_df[\"seat1\"] == \"none\") & (select_df[\"seat3\"] == \"toddler\") & (select_df[\"seat2\"] == \"none\"))]\n",
    "    return select_df\n",
    "\n",
    "#select only data that have one ore more adults alone in the back seats (OR NONE)\n",
    "def select_only_adult(df):\n",
    "    select_df = df.copy()\n",
    "    select_df = select_df[((select_df[\"seat1\"] == \"adult\") & (select_df[\"seat2\"] == \"none\") & (select_df[\"seat3\"] == \"none\")) \n",
    "                          | ((select_df[\"seat1\"] == \"none\") & (select_df[\"seat2\"] == \"adult\") & (select_df[\"seat3\"] == \"none\")) \n",
    "                          | ((select_df[\"seat1\"] == \"none\") & (select_df[\"seat2\"] == \"none\") & (select_df[\"seat3\"] == \"adult\"))\n",
    "                          | ((select_df[\"seat1\"] == \"adult\") & (select_df[\"seat2\"] == \"adult\") & (select_df[\"seat3\"] == \"none\"))\n",
    "                          | ((select_df[\"seat1\"] == \"none\") & (select_df[\"seat2\"] == \"adult\") & (select_df[\"seat3\"] == \"adult\"))\n",
    "                          | ((select_df[\"seat1\"] == \"adult\") & (select_df[\"seat2\"] == \"none\") & (select_df[\"seat3\"] == \"adult\"))\n",
    "                          | ((select_df[\"seat1\"] == \"adult\") & (select_df[\"seat2\"] == \"adult\") & (select_df[\"seat3\"] == \"adult\"))\n",
    "                          | ((select_df[\"seat1\"] == \"none\") & (select_df[\"seat2\"] == \"none\") & (select_df[\"seat3\"] == \"none\"))]\n",
    "    return select_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINE FUNCTIONS FOR ASSIGNING OCCUPANTS\n",
    "\n",
    "#assign occupations status of seats\n",
    "def assign_occupations(df):\n",
    "    for seat_number in range(1,4):\n",
    "        occ_seat = []\n",
    "        seat = 'seat' + str(seat_number)\n",
    "        for x in df[seat]:\n",
    "            if x != 'none':\n",
    "                occ_seat.append(1)\n",
    "            else:\n",
    "                occ_seat.append(0)\n",
    "        df['class' + str(seat_number)] = occ_seat\n",
    "\n",
    "#DEFINE PRESENCE AS AT LEAST 1 SEAT OCCUPIED\n",
    "def assign_presence(df):\n",
    "    presences = []\n",
    "    for index, row in df.iterrows():\n",
    "        presence = row['class1'] or row['class2'] or row['class3']\n",
    "        presences.append(presence)\n",
    "    df['presence'] = presences\n",
    "\n",
    "#ASSING NUMBER OF OCCUPANTS\n",
    "def assign_occupants(df):\n",
    "    occupants = []\n",
    "    for index, row in df.iterrows():\n",
    "        count = 0\n",
    "        count = row['class1'] + row['class2'] + row['class3']\n",
    "        if count == 3:\n",
    "            count = 2\n",
    "        occupants.append(count)\n",
    "    df['occupants'] = occupants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DETERMINE PRESENCE ON OUR DATASET\n",
    "assign_occupations(df)\n",
    "assign_occupants(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>occupants</th>\n",
       "      <th>class1</th>\n",
       "      <th>class2</th>\n",
       "      <th>class3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>478 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     occupants  class1  class2  class3\n",
       "0            1       0       0       1\n",
       "1            2       1       1       1\n",
       "2            2       1       0       1\n",
       "3            1       0       1       0\n",
       "4            2       1       1       0\n",
       "..         ...     ...     ...     ...\n",
       "473          1       0       0       1\n",
       "474          1       1       0       0\n",
       "475          1       1       0       0\n",
       "476          0       0       0       0\n",
       "477          0       0       0       0\n",
       "\n",
       "[478 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_info = complete_df[['occupants', 'class1', 'class2', 'class3']]\n",
    "df_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "478"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LENGHT OF THE NEWLY GENERATED DATASET\n",
    "len(complete_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class1</th>\n",
       "      <th>class2</th>\n",
       "      <th>class3</th>\n",
       "      <th>occupants</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>478.000000</td>\n",
       "      <td>478.000000</td>\n",
       "      <td>478.000000</td>\n",
       "      <td>478.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.539749</td>\n",
       "      <td>0.265690</td>\n",
       "      <td>0.382845</td>\n",
       "      <td>1.075314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.498940</td>\n",
       "      <td>0.442163</td>\n",
       "      <td>0.486590</td>\n",
       "      <td>0.767922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           class1      class2      class3   occupants\n",
       "count  478.000000  478.000000  478.000000  478.000000\n",
       "mean     0.539749    0.265690    0.382845    1.075314\n",
       "std      0.498940    0.442163    0.486590    0.767922\n",
       "min      0.000000    0.000000    0.000000    0.000000\n",
       "25%      0.000000    0.000000    0.000000    0.000000\n",
       "50%      1.000000    0.000000    0.000000    1.000000\n",
       "75%      1.000000    1.000000    1.000000    2.000000\n",
       "max      1.000000    1.000000    1.000000    2.000000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DESCRIPTION OF THE NEWLY GENERATED DATASET\n",
    "complete_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPLIT THE DATASET IN TRAIN AND TEST\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(complete_df, test_size=0.15, random_state = 42, stratify=complete_df.occupants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "406\n",
      "72\n"
     ]
    }
   ],
   "source": [
    "#LENGHT OF THE TEST DATASET\n",
    "print(len(train_df))\n",
    "print(len(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-105.20099314637616\n",
      "14.199503543044287\n",
      "(406, 53, 86, 1)\n",
      "0.25862068965517243\n",
      "0.4064039408866995\n",
      "0.33497536945812806\n"
     ]
    }
   ],
   "source": [
    "working_df = train_df\n",
    "#TRAINING WILL BE DONE WITH THE FFTDATA COLUMN\n",
    "train_list = working_df.fftData\n",
    "train_list = np.array(train_list)\n",
    "train_x = []\n",
    "\n",
    "\"\"\" Here is performed the frequency selection part of the preprocessing. \n",
    "    Since the fft spectrum is divided in two spectrum of 128 bits each, for performing frequency selection \n",
    "    we need to select from both the first part and the second one.\n",
    "    only one/fraction of the frequencies are selected.\n",
    "\"\"\"\n",
    "\n",
    "#Select only first third of both images\n",
    "fraction = 3 \n",
    "fraction_data = int(round(128/fraction)) #fraction_data=43 in this case\n",
    "\n",
    "for i in range(len(train_list)):\n",
    "    #print(len(train_list[i]))\n",
    "    #print(len(train_list[i][0]))\n",
    "    \n",
    "    a = np.array(train_list[i])[:, 0 : fraction_data]\n",
    "    b = np.array(train_list[i])[:, 128 : 128 + fraction_data]\n",
    "    c = np.concatenate((a, b), axis=1)\n",
    "    train_x.append(c)\n",
    "train_arr = []\n",
    "for x in range(len(train_x)):\n",
    "    train_arr.append(np.array(train_x[x]))\n",
    "train_list = train_arr \n",
    "\n",
    "\"\"\"\n",
    "zscore normalization part of the preprocessing. correcting the dimension of the network.\n",
    "\"\"\"\n",
    "\n",
    "print(np.mean(train_list))\n",
    "print(np.std(train_list))\n",
    "train_list = scipy.stats.zscore(train_list, axis=None)\n",
    "\n",
    "#max = np.max(train_list)\n",
    "#min = np.min(train_list)\n",
    "#train_list = np.array([[[(x - min) / (max - min) for x in y] for y in z] for z in train_list])\n",
    "\n",
    "\n",
    "train_tensor = tf.convert_to_tensor(train_list)\n",
    "\n",
    "#Third dimension value is 1\n",
    "train_tensor = tf.expand_dims(train_tensor, -1)\n",
    "\n",
    "print(train_tensor.shape)\n",
    "\n",
    "\"\"\"\n",
    "assigning label \n",
    "\"\"\"\n",
    "\n",
    "train_label = working_df[\"occupants\"]\n",
    "\n",
    "#PROPORTIONS OF THE DATASET\n",
    "passengers0 = 0\n",
    "passengers1 = 0\n",
    "passengers2 = 0\n",
    "#passengers3 = 0\n",
    "for occupants in working_df[\"occupants\"]:\n",
    "    if occupants == 0:\n",
    "        passengers0+=1\n",
    "    if occupants == 1:\n",
    "        passengers1+=1\n",
    "    if occupants == 2:\n",
    "        passengers2+=1\n",
    "    #if occupants == 3:\n",
    "        #passengers3+=1\n",
    "balancing0 = passengers0/len(train_df)\n",
    "balancing1 = passengers1/len(train_df)\n",
    "balancing2 = passengers2/len(train_df)\n",
    "#balancing3 = passengers3/len(train_df)\n",
    "#balancing = np.mean(working_df[\"occupants\"])\n",
    "print(balancing0)\n",
    "print(balancing1)\n",
    "print(balancing2)\n",
    "#print(balancing3)\n",
    "\n",
    "train_label = tf.keras.utils.to_categorical(train_label, 3)\n",
    "\n",
    "\"\"\"Dimensions of the inputs\"\"\"\n",
    "#53*86 images\n",
    "img_h, img_w = 53, fraction_data*2\n",
    "num_classes=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to run inference on a TFLite model\n",
    "def run_tflite_model(tflite_file, inputs, targets, test_image_indices):\n",
    "\n",
    "  # Initialize the interpreter\n",
    "    interpreter = tf.lite.Interpreter(model_path=str(tflite_file))\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    input_details = interpreter.get_input_details()[0]\n",
    "    output_details = interpreter.get_output_details()[0]\n",
    "\n",
    "    predictions = np.zeros((len(test_image_indices),), dtype=int)\n",
    "    for i, test_image_index in enumerate(test_image_indices):\n",
    "        #print(test_image_index)\n",
    "        test_image = inputs[test_image_index]\n",
    "        test_label = targets[test_image_index]\n",
    "\n",
    "        # Check if the input type is quantized, then rescale input data to uint8\n",
    "        #print(input_details['dtype'])\n",
    "        if input_details['dtype'] == np.int8:\n",
    "            #print(\"correct\")\n",
    "            input_scale, input_zero_point = input_details[\"quantization\"]\n",
    "            test_image = test_image / input_scale + input_zero_point\n",
    "\n",
    "        test_image = np.expand_dims(test_image, axis=0).astype(input_details[\"dtype\"])\n",
    "        interpreter.set_tensor(input_details[\"index\"], test_image)\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
    "        #print(output)\n",
    "        predictions[i] = output.argmax()\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(MP_dim2, conv_kernel_w2, conv_kernel_h2, conv_block2, drop_out_rate2, start_f2, regularizer, inputs, targets, dilation_rate, fraction):\n",
    "    # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training model with MP {MP_dim2}, conv kernel{[conv_kernel_w2, conv_kernel_h2]}, {conv_block2} blocks, {start_f2} filters, {drop_out_rate2} do...')\n",
    "    experiment = \"3_classes_test_4\" #¡¡¡¡¡CHANGE FOLDER!!!!!!\n",
    "    acc_per_fold = []\n",
    "    loss_per_fold = []\n",
    "    train_acc_per_fold = []\n",
    "    \n",
    "    acc_per_fold_quant = []\n",
    "    train_acc_per_fold_quant = []\n",
    "    \n",
    "    fold_no = 1\n",
    "    Y_pred_list = []\n",
    "    Y_true_list = []\n",
    "    \n",
    "    #--------------------------------STATIC PARAMETERS------------------------------------------\n",
    "    # Define the K-fold Cross Validator\n",
    "    kfold = KFold(n_splits=10, shuffle=True)\n",
    "    #loo = LeaveOneOut()\n",
    "    \n",
    "    \n",
    "    # Optimization params\n",
    "    # -------------------\n",
    "\n",
    "    # Loss\n",
    "    loss = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "    # learning rate\n",
    "    lr = 0.3e-4\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    # -------------------\n",
    "\n",
    "    # Validation metrics\n",
    "    # ------------------\n",
    "\n",
    "    metrics = ['accuracy']\n",
    "\n",
    "    batch_size = 32\n",
    "    \n",
    "    n_epoch = 400\n",
    "    \n",
    "    \n",
    "    #------------------------------------CALLBACKS----------------------------------------\n",
    "    callbacks = []\n",
    "    \n",
    "    # Early Stopping\n",
    "    # --------------\n",
    "    early_stop = False\n",
    "    if early_stop:\n",
    "        es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=100, restore_best_weights=False,)\n",
    "        callbacks.append(es_callback)\n",
    "    \n",
    "    # ----------------------------------CROSSVALIDATION-----------------------------------\n",
    "    for train, test in kfold.split(inputs, targets):\n",
    "      # Define the model architecture\n",
    "        print(f'{MP_dim2}, {[conv_kernel_w2, conv_kernel_h2]}, {conv_block2}, {start_f2}, {drop_out_rate2}, fold {fold_no}...')\n",
    "        model = tf.keras.Sequential()\n",
    "        \n",
    "        input_shape = [img_h, img_w, 1]\n",
    "\n",
    "        model.add(tf.keras.layers.MaxPool2D(pool_size=(MP_dim2, MP_dim2), input_shape=input_shape))\n",
    "        \n",
    "        n_filters = start_f2\n",
    "        \n",
    "        for i in range(conv_block2):\n",
    "            # Conv block: Conv2D -> Conv2D -> Activation -> Pooling\n",
    "            model.add(tf.keras.layers.Conv2D(filters=n_filters, \n",
    "                                             kernel_size=(conv_kernel_w2, conv_kernel_h2),\n",
    "                                             strides=(1, 1),\n",
    "                                             dilation_rate = dilation_rate,\n",
    "                                             padding='same'))\n",
    "            model.add(tf.keras.layers.Conv2D(filters=n_filters, \n",
    "                                             kernel_size=(conv_kernel_h2, conv_kernel_w2),\n",
    "                                             strides=(1, 1),\n",
    "                                             dilation_rate = dilation_rate,\n",
    "                                             padding='same',\n",
    "                                             activation = 'relu'))  \n",
    "            model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "        # Classifier\n",
    "        \n",
    "        model.add(tf.keras.layers.Flatten())\n",
    "        model.add(tf.keras.layers.Dropout(drop_out_rate2))\n",
    "        model.add(tf.keras.layers.Dense(units=num_classes, activation='softmax', kernel_regularizer=regularizer))\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(loss=loss,\n",
    "                    optimizer=optimizer,\n",
    "                    metrics=['accuracy'])\n",
    "        # Fit data to model\n",
    "        class_weights={0: balancing0, 1: balancing1, 2: balancing2}\n",
    "        history = model.fit(inputs[(train)], targets[(train)],\n",
    "                    class_weight=class_weights,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=n_epoch,\n",
    "                    validation_data=(inputs[(test)], targets[(test)]),\n",
    "                    callbacks = callbacks,\n",
    "                    verbose=False)\n",
    "\n",
    "        acc_per_fold.append(history.history['val_accuracy'])\n",
    "        loss_per_fold.append(history.history['val_loss'])\n",
    "        train_acc_per_fold.append(history.history['accuracy'])\n",
    "\n",
    "        Y_prediction = model.predict(inputs[test])\n",
    "        Y_pred_list.append(np.argmax(Y_prediction,axis = 1)) \n",
    "        Y_true_list.append(np.argmax( targets[test],axis = 1))\n",
    "        \n",
    "        #-------------------------------SAVE MODEL-----------------------------------------\n",
    "        \n",
    "        MODELS_DIR = f'models/experiment_{experiment}/fraction_{fraction}/n_epoch_{n_epoch}/{MP_dim2}-{conv_kernel_w2}-{conv_kernel_h2}-{conv_block2}-{start_f2}-{drop_out_rate2}-{dilation_rate}/'\n",
    "        try:\n",
    "            if not os.path.exists(MODELS_DIR):\n",
    "                os.makedirs(MODELS_DIR)\n",
    "        except e:\n",
    "            if e.errno != errno.EEXIST:\n",
    "                raise   \n",
    "            # time.sleep might help here\n",
    "            pass\n",
    "            \n",
    "        MODEL_TF = MODELS_DIR + f'fold_{fold_no}'\n",
    "        model.save(MODEL_TF)\n",
    "        # Increase fold number\n",
    "        \n",
    "        \n",
    "            #--------------------------QUANTIZE THE MODEL ----------------------------------\n",
    "        MODEL_TFLITE = MODELS_DIR + f'fold_{fold_no}.tflite'\n",
    "\n",
    "        def representative_dataset():\n",
    "            for data in tf.data.Dataset.from_tensor_slices((inputs)).batch(1).take(100):\n",
    "                #print(data)\n",
    "                yield [tf.dtypes.cast(data, tf.float32)]\n",
    "                \n",
    "\n",
    "        converter = tf.lite.TFLiteConverter.from_saved_model(MODEL_TF)\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        # Enforce integer only quantization\n",
    "        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "        converter.inference_input_type = tf.int8\n",
    "        converter.inference_output_type = tf.int8\n",
    "        # Provide a representative dataset to ensure we quantize correctly.\n",
    "        converter.representative_dataset = representative_dataset\n",
    "        model_tflite = converter.convert()\n",
    "\n",
    "        open(MODEL_TFLITE, \"wb\").write(model_tflite)\n",
    "\n",
    "        predictions = run_tflite_model(MODEL_TFLITE, inputs, targets, test)\n",
    "\n",
    "        accuracy = (np.sum(np.argmax( targets[test],axis = 1) == predictions) * 100) / len(test)\n",
    "        \n",
    "        predictions_train = run_tflite_model(MODEL_TFLITE, inputs, targets, train)\n",
    "        accuracy_train = (np.sum(np.argmax( targets[train],axis = 1) == predictions_train) * 100) / len(train)\n",
    "        \n",
    "        acc_per_fold_quant.append(accuracy)\n",
    "        train_acc_per_fold_quant.append(accuracy_train)\n",
    "        print(f\"accuracy: {np.array(acc_per_fold)[:, -1]}\")\n",
    "        print(f\"quantized accuracy: {acc_per_fold_quant}\")\n",
    "        #print(train_acc_per_fold_quant)\"\"\"\n",
    "        fold_no = fold_no + 1\n",
    "    \n",
    "    #---------------------------SAVE RESULTS TO JSON---------------------------------------\n",
    "    Y_true = []\n",
    "    Y_pred = []\n",
    "    for i in range(len(Y_pred_list)):\n",
    "        Y_true = np.concatenate((Y_true, Y_true_list[i]))\n",
    "        Y_pred = np.concatenate((Y_pred, Y_pred_list[i]))\n",
    "    row = {'MaxPoolDim' : MP_dim2, \n",
    "           'conv_kernel_dim' : [conv_kernel_w2, conv_kernel_h2], \n",
    "           'n_conv' : conv_block2, \n",
    "           'n_filters' : start_f2, \n",
    "           'dropout' : drop_out_rate2, \n",
    "           'n_epochs' : n_epoch,\n",
    "           'dilation_rate' : dilation_rate,\n",
    "           'train_accuracy' : np.mean(train_acc_per_fold, axis=0).tolist(),\n",
    "           'valid_accuracy' : np.mean(acc_per_fold, axis=0).tolist(),\n",
    "           'Y_true' : Y_true.tolist(), \n",
    "           'Y_pred' : Y_pred.tolist(),\n",
    "           'train_accuracy_quant':  np.mean(train_acc_per_fold_quant), \n",
    "           'valid_accuracy_quant':  np.mean(acc_per_fold_quant)\n",
    "          }\n",
    "    JSON_DIR = f'json_child/experiment_{experiment}/fraction_{fraction}/n_epoch_{n_epoch}'\n",
    "    if not os.path.exists(JSON_DIR):\n",
    "        os.makedirs(JSON_DIR)\n",
    "    try:\n",
    "        with open(f'{JSON_DIR}/{MP_dim2}-{conv_kernel_w2}-{conv_kernel_h2}-{conv_block2}-{start_f2}-{drop_out_rate2}-{dilation_rate}.json', 'w') as f:\n",
    "            json.dump(row, f)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "    \n",
    "    #--------------------------PLOT ACCURACIES CURVES ------------------------------\n",
    "    plt.clf()\n",
    "    plt.plot(np.mean(train_acc_per_fold, axis=0))\n",
    "    plt.plot(np.mean(acc_per_fold, axis=0))\n",
    "    plt.savefig(f'{JSON_DIR}/{MP_dim2}-{conv_kernel_w2}-{conv_kernel_h2}-{conv_block2}-{start_f2}-{drop_out_rate2}-{dilation_rate}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    \n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162 will be trained\n"
     ]
    }
   ],
   "source": [
    "# Merge inputs and targets\n",
    "\n",
    "inputs = np.array(train_tensor)\n",
    "targets = train_label\n",
    "\n",
    "#-----------------------------------GRID SEARCH PARAMETERS ---------------------------\n",
    "\n",
    "#firts MaxPool dimension, to reduce the input size.\n",
    "MP_dims = [2]\n",
    "\n",
    "#kernel dimension of filters. \n",
    "conv_kernels = [[3,3],[5,5],[7,7]]\n",
    "\n",
    "#number of convolution blocks\n",
    "conv_blocks = [2, 3, 4] \n",
    "\n",
    "#Dropout rate\n",
    "drop_out_rates = [0.3]\n",
    "\n",
    "#number of filters in each convolutional layer\n",
    "start_fs =  [6, 8, 10, 12, 14, 16, 18, 20, 22]\n",
    "\n",
    "#regularizers to use in the network\n",
    "regularizers = [None]#, regularizers.l2(1e-4), regularizers.l2(1e-5), regularizers.l2(1e-3)]\n",
    "\n",
    "#dilation rates of the convolutional layers\n",
    "dilation_rates = [1, 2]\n",
    "\n",
    "n_models = len(MP_dims) * len(conv_kernels) * len(conv_blocks) * len(start_fs) * len(drop_out_rates) * len(dilation_rates)\n",
    "\n",
    "print(f\"{n_models} will be trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one(arg):\n",
    "    train_network(arg[0], arg[1], arg[2],arg[3],arg[4], arg[5], arg[6], arg[7], arg[8], arg[9], arg[10])\n",
    "\n",
    "def train_all(args):\n",
    "    for elem in args:\n",
    "        train_one(elem)\n",
    "    return 0\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = []\n",
    "    for MP_dim in MP_dims:\n",
    "        for conv_kernel in conv_kernels:\n",
    "            for conv_block in conv_blocks:\n",
    "                for drop_out_rate in drop_out_rates:\n",
    "                    for start_f in start_fs:\n",
    "                        for regularizer in regularizers:\n",
    "                            for dilation_rate in dilation_rates:\n",
    "                                args.append([MP_dim, conv_kernel[0], conv_kernel[1], conv_block, drop_out_rate, start_f, regularizer, inputs, targets, dilation_rate, fraction])\n",
    "\n",
    "    results = train_all(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"retrain the best performing network on train + validation dataset\"\"\" \n",
    "accuracies=[]\n",
    "def train_random_states(amount):\n",
    "    for i in range(amount):\n",
    "        print('----------------------------------------------------------------------------')\n",
    "        print(f'RND STATE NUMBER = {i}')\n",
    "        print('----------------------------------------------------------------------------')\n",
    "        \"\"\"SPLIT THE DATASET\"\"\"\n",
    "        from sklearn.model_selection import train_test_split\n",
    "\n",
    "        train_df, test_df = train_test_split(df, test_size=0.15, random_state = i)\n",
    "        \n",
    "        \"\"\"DATA PREPROCESSING\"\"\"\n",
    "        working_df = train_df\n",
    "        #TRAINING WILL BE DONE WITH THE FFTDATA COLUMN\n",
    "        train_list = working_df.fftData\n",
    "        train_list = np.array(train_list)\n",
    "        train_x = []\n",
    "\n",
    "        \"\"\" Here is performed the frequency selection part of the preprocessing. \n",
    "            Since the fft spectrum is divided in two spectrum of 128 bits each, for performing frequency selection \n",
    "            we need to select from both the first part and the second one.\n",
    "            only one/fraction of the frequencies are selected.\n",
    "        \"\"\"\n",
    "\n",
    "        #Select only first third of both images\n",
    "        fraction = 3 \n",
    "        fraction_data = int(round(128/fraction)) #fraction_data=43 in this case\n",
    "\n",
    "        for i in range(len(train_list)):\n",
    "            #print(len(train_list[i]))\n",
    "            #print(len(train_list[i][0]))\n",
    "\n",
    "            a = np.array(train_list[i])[:, 0 : fraction_data]\n",
    "            b = np.array(train_list[i])[:, 128 : 128 + fraction_data]\n",
    "            c = np.concatenate((a, b), axis=1)\n",
    "            train_x.append(c)\n",
    "        train_arr = []\n",
    "        for x in range(len(train_x)):\n",
    "            train_arr.append(np.array(train_x[x]))\n",
    "        train_list = train_arr \n",
    "\n",
    "        \"\"\"\n",
    "        zscore normalization part of the preprocessing. correcting the dimension of the network.\n",
    "        \"\"\"\n",
    "\n",
    "        print(np.mean(train_list))\n",
    "        print(np.std(train_list))\n",
    "        train_list = scipy.stats.zscore(train_list, axis=None)\n",
    "\n",
    "        #max = np.max(train_list)\n",
    "        #min = np.min(train_list)\n",
    "        #train_list = np.array([[[(x - min) / (max - min) for x in y] for y in z] for z in train_list])\n",
    "\n",
    "\n",
    "        train_tensor = tf.convert_to_tensor(train_list)\n",
    "\n",
    "        #Third dimension value is 1\n",
    "        train_tensor = tf.expand_dims(train_tensor, -1)\n",
    "\n",
    "        print(train_tensor.shape)\n",
    "\n",
    "        \"\"\"\n",
    "        assigning label \n",
    "        \"\"\"\n",
    "\n",
    "        train_label = working_df[\"occupants\"]\n",
    "\n",
    "        #PROPORTIONS OF THE DATASET\n",
    "        passengers0 = 0\n",
    "        passengers1 = 0\n",
    "        passengers2 = 0\n",
    "        #passengers3 = 0\n",
    "        for occupants in working_df[\"occupants\"]:\n",
    "            if occupants == 0:\n",
    "                passengers0+=1\n",
    "            if occupants == 1:\n",
    "                passengers1+=1\n",
    "            if occupants == 2:\n",
    "                passengers2+=1\n",
    "            #if occupants == 3:\n",
    "                #passengers3+=1\n",
    "        balancing0 = passengers0/len(train_df)\n",
    "        balancing1 = passengers1/len(train_df)\n",
    "        balancing2 = passengers2/len(train_df)\n",
    "        #balancing3 = passengers3/len(train_df)\n",
    "        #balancing = np.mean(working_df[\"occupants\"])\n",
    "        print(balancing0)\n",
    "        print(balancing1)\n",
    "        print(balancing2)\n",
    "        #print(balancing3)\n",
    "\n",
    "        train_label = tf.keras.utils.to_categorical(train_label, 3)\n",
    "\n",
    "        \"\"\"Dimensions of the inputs\"\"\"\n",
    "        #53*86 images\n",
    "        img_h, img_w = 53, fraction_data*2\n",
    "        num_classes=3\n",
    "        \n",
    "        \"\"\"\n",
    "        Perform the same preprocessing steps of the training set to the test set too.\n",
    "        \"\"\"\n",
    "        test_labels = np.array(test_df[\"occupants\"])\n",
    "\n",
    "        test_list = test_df[\"fftData\"]\n",
    "        test_list = np.array(test_list)\n",
    "        test_x = []\n",
    "\n",
    "        fraction = 3 \n",
    "        fraction_data = int(round(128/fraction))\n",
    "\n",
    "        for i in range(len(test_list)):\n",
    "\n",
    "            a = np.array(test_list[i])[:, 0 : fraction_data]\n",
    "            b = np.array(test_list[i])[:, 128 : 128 + fraction_data]\n",
    "            c = np.concatenate((a, b), axis=1)\n",
    "            test_x.append(c)\n",
    "        test_arr = []\n",
    "        for x in range(len(test_x)):\n",
    "            test_arr.append(np.array(test_x[x]))\n",
    "        test_list = test_arr \n",
    "\n",
    "\n",
    "        print(np.mean(test_list))\n",
    "        print(np.std(test_list))\n",
    "        test_list = scipy.stats.zscore(test_list, axis=None)\n",
    "\n",
    "        #max = np.max(train_list)\n",
    "        #min = np.min(train_list)\n",
    "        #train_list = np.array([[[(x - min) / (max - min) for x in y] for y in z] for z in train_list])\n",
    "        test_tensor = tf.convert_to_tensor(test_list)\n",
    "        test_tensor = tf.expand_dims(test_tensor, -1)\n",
    "        print(test_tensor.shape)\n",
    "        test_images = test_tensor\n",
    "\n",
    "        \"\"\"\n",
    "        test the best performing network on the test set.\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"NETWORK DESIGN\"\"\"\n",
    "        # Loss\n",
    "        loss = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "        # learning rate\n",
    "        lr = 0.3e-4\n",
    "\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "        # -------------------\n",
    "\n",
    "        # Validation metrics\n",
    "        # ------------------\n",
    "\n",
    "        metrics = ['accuracy']\n",
    "\n",
    "        batch_size = 32\n",
    "\n",
    "        callbacks = []\n",
    "\n",
    "        n_epoch = 300\n",
    "\n",
    "        conv_kernels = [7]\n",
    "\n",
    "        MP_dims = [2]\n",
    "\n",
    "        #number of convolution blocks\n",
    "        conv_blocks = [2]\n",
    "\n",
    "        #Dropout rate\n",
    "        drop_out_rates = [0.3]\n",
    "\n",
    "        #numbers of starting filters\n",
    "        start_fs = [22]\n",
    "\n",
    "        #dilation_rate\n",
    "        dilation_rates = [1]\n",
    "\n",
    "        regularizer = None\n",
    "\n",
    "        for MP_dim in MP_dims:\n",
    "            for conv_kernel in conv_kernels:\n",
    "                for conv_block in conv_blocks:\n",
    "                    for drop_out_rate in drop_out_rates:\n",
    "                        for n_filter in start_fs:\n",
    "                            for dilation_rate in dilation_rates:\n",
    "\n",
    "                                experiment = f\"3_classes_BEST_{i}\"\n",
    "\n",
    "                                model = tf.keras.Sequential()\n",
    "\n",
    "                                input_shape = [img_h, img_w, 1]\n",
    "\n",
    "\n",
    "                                model = tf.keras.Sequential()\n",
    "\n",
    "                                model.add(tf.keras.layers.MaxPool2D(pool_size=(MP_dim, MP_dim), input_shape=input_shape))\n",
    "\n",
    "                                for i in range(conv_block):\n",
    "                                    # Conv block: Conv2D -> Activation -> Pooling\n",
    "                                    model.add(tf.keras.layers.Conv2D(filters=n_filter, \n",
    "                                                                     kernel_size=(conv_kernel, conv_kernel),\n",
    "                                                                     strides=(1, 1),\n",
    "                                                                     dilation_rate = dilation_rate,\n",
    "                                                                     padding='same',\n",
    "                                                                     input_shape=input_shape))   \n",
    "                                    model.add(tf.keras.layers.Conv2D(filters=n_filter, \n",
    "                                                                     kernel_size=(conv_kernel, conv_kernel),\n",
    "                                                                     strides=(1, 1),\n",
    "                                                                     dilation_rate = dilation_rate,\n",
    "                                                                     padding='same',\n",
    "                                                                     input_shape=input_shape,\n",
    "                                                                     activation='relu'))\n",
    "                                    model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "                                # Classifier\n",
    "\n",
    "                                model.add(tf.keras.layers.Flatten())\n",
    "                                model.add(tf.keras.layers.Dropout(drop_out_rate))\n",
    "                                model.add(tf.keras.layers.Dense(units=num_classes, activation='sigmoid', kernel_regularizer=regularizer))\n",
    "\n",
    "                                # Compile the model\n",
    "                                model.compile(loss=loss,\n",
    "                                            optimizer=optimizer,\n",
    "                                            metrics=['accuracy'])\n",
    "\n",
    "\n",
    "                                # Generate a print\n",
    "                                print('------------------------------------------------------------------------')\n",
    "\n",
    "                                # Fit data to model\n",
    "                                class_weights={0: balancing0, 1: balancing1, 2: balancing2}\n",
    "                                history = model.fit(inputs, targets,\n",
    "                                            class_weight=class_weights,\n",
    "                                            batch_size=batch_size,\n",
    "                                            epochs=n_epoch,\n",
    "                                            callbacks = callbacks,\n",
    "                                            verbose=False)\n",
    "\n",
    "                                MODELS_DIR = f'models_full_train/experiment_{experiment}/experiment_{experiment}_{i}/{MP_dim}-{conv_kernel}-{conv_block}-{n_filter}-{drop_out_rate}-{dilation_rate}/'\n",
    "                                if not os.path.exists(MODELS_DIR):\n",
    "                                    os.makedirs(MODELS_DIR)\n",
    "                                MODEL_TF = MODELS_DIR + f'model'\n",
    "                                MODEL_TFLITE_MICRO = MODELS_DIR + 'model.cc'\n",
    "                                MODEL_TFLITE = MODELS_DIR + 'model.tflite'\n",
    "                                model.save(MODEL_TF)\n",
    "\n",
    "                                def representative_dataset():\n",
    "                                    for data in tf.data.Dataset.from_tensor_slices((inputs)).batch(1).take(100):\n",
    "                                        #print(data)\n",
    "                                        yield [tf.dtypes.cast(data, tf.float32)]\n",
    "\n",
    "\n",
    "                                converter = tf.lite.TFLiteConverter.from_saved_model(MODEL_TF)\n",
    "                                converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "                                # Enforce integer only quantization\n",
    "                                converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "                                converter.inference_input_type = tf.int8\n",
    "                                converter.inference_output_type = tf.int8\n",
    "                                # Provide a representative dataset to ensure we quantize correctly.\n",
    "                                converter.representative_dataset = representative_dataset\n",
    "                                model_tflite = converter.convert()\n",
    "\n",
    "                                open(MODEL_TFLITE, \"wb\").write(model_tflite)\n",
    "                                #open(MODEL_TFLITE2, \"wb\").write(model_tflite)\n",
    "                                \n",
    "        outs = model(test_images)#, test_labels, verbose=2)\n",
    "        predicted = np.argmax(outs, axis=1)\n",
    "        print(predicted)\n",
    "        print(test_labels)\n",
    "\n",
    "        from sklearn.metrics import confusion_matrix\n",
    "        conf = confusion_matrix(test_labels, predicted)\n",
    "        accuracy = 0\n",
    "        top = 0\n",
    "        bottom = 0\n",
    "        for j in range(0,len(conf)):\n",
    "            top += conf[j][j]\n",
    "            for k in range(0,len(conf[j])):\n",
    "                bottom += conf[j][k]\n",
    "        accuracy = top/bottom\n",
    "        print(f\"accuracy of experiment_{experiment}_{i} = {accuracy}\")\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "    row = {'accuracy' : accuracies}\n",
    "    JSON_DIR = f'json_child/experiment_3_classes_BEST/fraction_{fraction}/n_epoch_300'\n",
    "    if not os.path.exists(JSON_DIR):\n",
    "        os.makedirs(JSON_DIR)\n",
    "    try:\n",
    "        with open(f'{JSON_DIR}/accuracies.json', 'w') as f:\n",
    "            json.dump(row, f)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_random_states(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results are saved on a .json file "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
