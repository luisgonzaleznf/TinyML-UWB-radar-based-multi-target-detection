{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import ast\n",
    "import scipy\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD 516 DATASET FOR FIRST TRAINING AND VALIDATION\n",
    "dataframe_name = \"dataframe516\"\n",
    "pkl_path = f\"./pickle/{dataframe_name}.pkl\"\n",
    "df = pd.read_pickle(pkl_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINE FUNCTIONS FOR SELECTING PARTS OF THE DATASET REGARDING DIFFERENT CONFIGURATIONS OF OCCUPANTS\n",
    "\n",
    "#select only data that have on seat1 a children or empty\n",
    "def select_only_children_on_seat1(df):\n",
    "    select_df = df.copy()\n",
    "    select_df = select_df[(select_df[\"seat1\"] == \"toddler\") | (select_df[\"seat1\"] == \"baby\") | (select_df[\"seat1\"] == \"none\")]\n",
    "    return select_df\n",
    "\n",
    "#select only data that have one ore more pets alone in the back seats (OR NONE)\n",
    "def select_only_pet(df):\n",
    "    select_df = df.copy()\n",
    "    select_df = select_df[((select_df[\"seat1\"] == \"pet\") & (select_df[\"seat2\"] == \"none\") & (select_df[\"seat3\"] == \"none\")) \n",
    "                          | ((select_df[\"seat1\"] == \"none\") & (select_df[\"seat2\"] == \"pet\") & (select_df[\"seat3\"] == \"none\")) \n",
    "                          | ((select_df[\"seat1\"] == \"none\") & (select_df[\"seat2\"] == \"none\") & (select_df[\"seat3\"] == \"pet\"))\n",
    "                          | ((select_df[\"seat1\"] == \"pet\") & (select_df[\"seat2\"] == \"pet\") & (select_df[\"seat3\"] == \"none\"))\n",
    "                          | ((select_df[\"seat1\"] == \"none\") & (select_df[\"seat2\"] == \"pet\") & (select_df[\"seat3\"] == \"pet\"))\n",
    "                          | ((select_df[\"seat1\"] == \"pet\") & (select_df[\"seat2\"] == \"none\") & (select_df[\"seat3\"] == \"pet\"))\n",
    "                          | ((select_df[\"seat1\"] == \"pet\") & (select_df[\"seat2\"] == \"pet\") & (select_df[\"seat3\"] == \"pet\"))\n",
    "                          | ((select_df[\"seat1\"] == \"none\") & (select_df[\"seat2\"] == \"none\") & (select_df[\"seat3\"] == \"none\"))]\n",
    "    return select_df\n",
    "\n",
    "#select only data that have only one target, adult or toddler, in the back seats (OR NONE)\n",
    "def select_only_single(df):\n",
    "    select_df = df.copy()\n",
    "    select_df = select_df[((select_df[\"seat1\"] == \"adult\") & (select_df[\"seat2\"] == \"none\") & (select_df[\"seat3\"] == \"none\")) \n",
    "                          | ((select_df[\"seat1\"] == \"none\") & (select_df[\"seat2\"] == \"adult\") & (select_df[\"seat3\"] == \"none\")) \n",
    "                          | ((select_df[\"seat1\"] == \"none\") & (select_df[\"seat3\"] == \"adult\") & (select_df[\"seat2\"] == \"none\"))\n",
    "                          | ((select_df[\"seat1\"] == \"toddler\") & (select_df[\"seat2\"] == \"none\") & (select_df[\"seat3\"] == \"none\")) \n",
    "                          | ((select_df[\"seat1\"] == \"none\") & (select_df[\"seat2\"] == \"toddler\") & (select_df[\"seat3\"] == \"none\")) \n",
    "                          | ((select_df[\"seat1\"] == \"none\") & (select_df[\"seat3\"] == \"toddler\") & (select_df[\"seat2\"] == \"none\"))\n",
    "                          | ((select_df[\"seat1\"] == \"none\") & (select_df[\"seat3\"] == \"none\") & (select_df[\"seat2\"] == \"none\"))]\n",
    "    return select_df\n",
    "\n",
    "#select only data that have only one target, adult or toddler, in the back seats. (NO NONE) (LUIS)\n",
    "def select_only_single_true(df):\n",
    "    select_df = df.copy()\n",
    "    select_df = select_df[((select_df[\"seat1\"] == \"adult\") & (select_df[\"seat2\"] == \"none\") & (select_df[\"seat3\"] == \"none\")) \n",
    "                          | ((select_df[\"seat1\"] == \"none\") & (select_df[\"seat2\"] == \"adult\") & (select_df[\"seat3\"] == \"none\")) \n",
    "                          | ((select_df[\"seat1\"] == \"none\") & (select_df[\"seat3\"] == \"adult\") & (select_df[\"seat2\"] == \"none\"))\n",
    "                          | ((select_df[\"seat1\"] == \"toddler\") & (select_df[\"seat2\"] == \"none\") & (select_df[\"seat3\"] == \"none\")) \n",
    "                          | ((select_df[\"seat1\"] == \"none\") & (select_df[\"seat2\"] == \"toddler\") & (select_df[\"seat3\"] == \"none\")) \n",
    "                          | ((select_df[\"seat1\"] == \"none\") & (select_df[\"seat3\"] == \"toddler\") & (select_df[\"seat2\"] == \"none\"))]\n",
    "    return select_df\n",
    "\n",
    "#select only data that have one ore more adults alone in the back seats (OR NONE) (LUIS)\n",
    "def select_only_adult(df):\n",
    "    select_df = df.copy()\n",
    "    select_df = select_df[((select_df[\"seat1\"] == \"adult\") & (select_df[\"seat2\"] == \"none\") & (select_df[\"seat3\"] == \"none\")) \n",
    "                          | ((select_df[\"seat1\"] == \"none\") & (select_df[\"seat2\"] == \"adult\") & (select_df[\"seat3\"] == \"none\")) \n",
    "                          | ((select_df[\"seat1\"] == \"none\") & (select_df[\"seat2\"] == \"none\") & (select_df[\"seat3\"] == \"adult\"))\n",
    "                          | ((select_df[\"seat1\"] == \"adult\") & (select_df[\"seat2\"] == \"adult\") & (select_df[\"seat3\"] == \"none\"))\n",
    "                          | ((select_df[\"seat1\"] == \"none\") & (select_df[\"seat2\"] == \"adult\") & (select_df[\"seat3\"] == \"adult\"))\n",
    "                          | ((select_df[\"seat1\"] == \"adult\") & (select_df[\"seat2\"] == \"none\") & (select_df[\"seat3\"] == \"adult\"))\n",
    "                          | ((select_df[\"seat1\"] == \"adult\") & (select_df[\"seat2\"] == \"adult\") & (select_df[\"seat3\"] == \"adult\"))\n",
    "                          | ((select_df[\"seat1\"] == \"none\") & (select_df[\"seat2\"] == \"none\") & (select_df[\"seat3\"] == \"none\"))]\n",
    "    return select_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINE FUNCTIONS FOR ASSIGNING OCCUPANTS\n",
    "\n",
    "#assign occupations status of seats\n",
    "def assign_occupations(df):\n",
    "    for seat_number in range(1,4):\n",
    "        occ_seat = []\n",
    "        seat = 'seat' + str(seat_number)\n",
    "        for x in df[seat]:\n",
    "            if x != 'none':\n",
    "                occ_seat.append(1)\n",
    "            else:\n",
    "                occ_seat.append(0)\n",
    "        df['class' + str(seat_number)] = occ_seat\n",
    "\n",
    "#DEFINE PRESENCE AS AT LEAST 1 SEAT OCCUPIED\n",
    "def assign_presence(df):\n",
    "    presences = []\n",
    "    for index, row in df.iterrows():\n",
    "        presence = row['class1'] or row['class2'] or row['class3']\n",
    "        presences.append(presence)\n",
    "    df['presence'] = presences\n",
    "\n",
    "#ASSING NUMBER OF OCCUPANTS (LUIS)\n",
    "def assign_occupants(df):\n",
    "    occupants = []\n",
    "    for index, row in df.iterrows():\n",
    "        count = 0\n",
    "        count = row['class1'] + row['class2'] + row['class3']\n",
    "        if count == 3:\n",
    "            count = 2\n",
    "        occupants.append(count)\n",
    "    df['occupants'] = occupants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>deviceSerial</th>\n",
       "      <th>fWversion</th>\n",
       "      <th>batteryLevel</th>\n",
       "      <th>hWversion</th>\n",
       "      <th>rawData</th>\n",
       "      <th>fftData</th>\n",
       "      <th>SW Version</th>\n",
       "      <th>seat1</th>\n",
       "      <th>seat2</th>\n",
       "      <th>...</th>\n",
       "      <th>vehicle</th>\n",
       "      <th>temperature</th>\n",
       "      <th>accX</th>\n",
       "      <th>accY</th>\n",
       "      <th>accZ</th>\n",
       "      <th>createdAt</th>\n",
       "      <th>class1</th>\n",
       "      <th>class2</th>\n",
       "      <th>class3</th>\n",
       "      <th>occupants</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>090cu2xvt8o7Gmx9sMFx</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100</td>\n",
       "      <td>1.2</td>\n",
       "      <td>{\"real\":[[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...</td>\n",
       "      <td>[[-105.87874280268608, -102.26725079789395, -1...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>...</td>\n",
       "      <td>FYMCwQblEjJrCt5S9KDV</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{'_seconds': 1603708000, '_nanoseconds': 54000...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0bv8HBSz1AHA8Mhp5qLm</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100</td>\n",
       "      <td>1.2</td>\n",
       "      <td>{\"real\":[[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...</td>\n",
       "      <td>[[-103.23020764266907, -100.18002761432678, -1...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>toddler</td>\n",
       "      <td>adult</td>\n",
       "      <td>...</td>\n",
       "      <td>4GImmlvjC676xXRyFECn</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{'_seconds': 1607352646, '_nanoseconds': 12600...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19xTU1eBTiK4mBieg8T2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100</td>\n",
       "      <td>1.2</td>\n",
       "      <td>{\"real\":[[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...</td>\n",
       "      <td>[[-107.26817809590841, -103.572805055402, -110...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pet</td>\n",
       "      <td>none</td>\n",
       "      <td>...</td>\n",
       "      <td>4GImmlvjC676xXRyFECn</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{'_seconds': 1604935813, '_nanoseconds': 52500...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1eEVvANdi96NtzXSDT1C</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100</td>\n",
       "      <td>1.2</td>\n",
       "      <td>{\"real\":[[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...</td>\n",
       "      <td>[[-108.10489947686006, -105.49311958297311, -1...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>pet</td>\n",
       "      <td>...</td>\n",
       "      <td>4GImmlvjC676xXRyFECn</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{'_seconds': 1604936210, '_nanoseconds': 91100...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2EttR31aXRBWqx8dRzgq</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100</td>\n",
       "      <td>1.2</td>\n",
       "      <td>{\"real\":[[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...</td>\n",
       "      <td>[[-117.84473965177918, -105.55119981094282, -1...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>toddler</td>\n",
       "      <td>baby</td>\n",
       "      <td>...</td>\n",
       "      <td>4GImmlvjC676xXRyFECn</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>{'_seconds': 1607771892, '_nanoseconds': 46000...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>z6yKyTEjlNMoFUpBlCxg</td>\n",
       "      <td>0.8.0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{\"real\":[[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...</td>\n",
       "      <td>[[-74.87174084241651, -70.40210124639611, -77....</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>...</td>\n",
       "      <td>emS4JG4Hp2H8d2Ab4ofV</td>\n",
       "      <td>None</td>\n",
       "      <td>-0.936</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>{'_seconds': 1636904824, '_nanoseconds': 43000...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>zBzfzVgNdiH5EHoPWPjz</td>\n",
       "      <td>0.8.0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{\"real\":[[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...</td>\n",
       "      <td>[[-68.41237424396255, -65.80510348327765, -79....</td>\n",
       "      <td>NaN</td>\n",
       "      <td>baby</td>\n",
       "      <td>none</td>\n",
       "      <td>...</td>\n",
       "      <td>6a9YEoNDIb7J4j5yNXVc</td>\n",
       "      <td>None</td>\n",
       "      <td>-0.676</td>\n",
       "      <td>-0.172</td>\n",
       "      <td>-0.58</td>\n",
       "      <td>{'_seconds': 1636898959, '_nanoseconds': 98800...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>zHvZrXdGjZB73NoeNGOT</td>\n",
       "      <td>0.8.0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{\"real\":[[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...</td>\n",
       "      <td>[[-86.55813594069244, -77.12291636247366, -78....</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adult</td>\n",
       "      <td>none</td>\n",
       "      <td>...</td>\n",
       "      <td>X1ILHgwtdueiSTYr8XdK</td>\n",
       "      <td>None</td>\n",
       "      <td>-0.896</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>-0.292</td>\n",
       "      <td>{'_seconds': 1636483781, '_nanoseconds': 19300...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>zII9nav2JaUQCyxzLKy5</td>\n",
       "      <td>0.8.0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{\"real\":[[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...</td>\n",
       "      <td>[[-89.49105064688271, -86.80552730271502, -95....</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>...</td>\n",
       "      <td>HAETRoo2NZj9oKG8JBAB</td>\n",
       "      <td>None</td>\n",
       "      <td>1.06</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.504</td>\n",
       "      <td>{'_seconds': 1620400379, '_nanoseconds': 62000...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>zZNHJm0aBsJjAiNuAVoi</td>\n",
       "      <td>0.8.0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{\"real\":[[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...</td>\n",
       "      <td>[[-98.65451677423928, -91.73755589282831, -102...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>...</td>\n",
       "      <td>4GImmlvjC676xXRyFECn</td>\n",
       "      <td>None</td>\n",
       "      <td>0.648</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.688</td>\n",
       "      <td>{'_seconds': 1624128012, '_nanoseconds': 29500...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>478 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Id deviceSerial fWversion batteryLevel hWversion  \\\n",
       "0    090cu2xvt8o7Gmx9sMFx            2       1.0          100       1.2   \n",
       "1    0bv8HBSz1AHA8Mhp5qLm            2       1.0          100       1.2   \n",
       "2    19xTU1eBTiK4mBieg8T2            2       1.0          100       1.2   \n",
       "3    1eEVvANdi96NtzXSDT1C            2       1.0          100       1.2   \n",
       "4    2EttR31aXRBWqx8dRzgq            2       1.0          100       1.2   \n",
       "..                    ...          ...       ...          ...       ...   \n",
       "473  z6yKyTEjlNMoFUpBlCxg      0.8.0.0      None          100       1.0   \n",
       "474  zBzfzVgNdiH5EHoPWPjz      0.8.0.0      None          100       1.0   \n",
       "475  zHvZrXdGjZB73NoeNGOT      0.8.0.0      None          100       1.0   \n",
       "476  zII9nav2JaUQCyxzLKy5      0.8.0.0      None          100       1.0   \n",
       "477  zZNHJm0aBsJjAiNuAVoi      0.8.0.0         1          100       1.0   \n",
       "\n",
       "                                               rawData  \\\n",
       "0    {\"real\":[[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...   \n",
       "1    {\"real\":[[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...   \n",
       "2    {\"real\":[[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...   \n",
       "3    {\"real\":[[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...   \n",
       "4    {\"real\":[[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...   \n",
       "..                                                 ...   \n",
       "473  {\"real\":[[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...   \n",
       "474  {\"real\":[[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...   \n",
       "475  {\"real\":[[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...   \n",
       "476  {\"real\":[[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...   \n",
       "477  {\"real\":[[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...   \n",
       "\n",
       "                                               fftData SW Version    seat1  \\\n",
       "0    [[-105.87874280268608, -102.26725079789395, -1...        NaN     none   \n",
       "1    [[-103.23020764266907, -100.18002761432678, -1...        NaN  toddler   \n",
       "2    [[-107.26817809590841, -103.572805055402, -110...        NaN      pet   \n",
       "3    [[-108.10489947686006, -105.49311958297311, -1...        NaN     none   \n",
       "4    [[-117.84473965177918, -105.55119981094282, -1...        NaN  toddler   \n",
       "..                                                 ...        ...      ...   \n",
       "473  [[-74.87174084241651, -70.40210124639611, -77....        NaN     none   \n",
       "474  [[-68.41237424396255, -65.80510348327765, -79....        NaN     baby   \n",
       "475  [[-86.55813594069244, -77.12291636247366, -78....        NaN    adult   \n",
       "476  [[-89.49105064688271, -86.80552730271502, -95....        NaN     none   \n",
       "477  [[-98.65451677423928, -91.73755589282831, -102...        NaN     none   \n",
       "\n",
       "     seat2  ...               vehicle temperature   accX   accY   accZ  \\\n",
       "0     none  ...  FYMCwQblEjJrCt5S9KDV        None   None   None   None   \n",
       "1    adult  ...  4GImmlvjC676xXRyFECn        None   None   None   None   \n",
       "2     none  ...  4GImmlvjC676xXRyFECn        None   None   None   None   \n",
       "3      pet  ...  4GImmlvjC676xXRyFECn        None   None   None   None   \n",
       "4     baby  ...  4GImmlvjC676xXRyFECn        None   None   None   None   \n",
       "..     ...  ...                   ...         ...    ...    ...    ...   \n",
       "473   none  ...  emS4JG4Hp2H8d2Ab4ofV        None -0.936 -0.072 -0.156   \n",
       "474   none  ...  6a9YEoNDIb7J4j5yNXVc        None -0.676 -0.172  -0.58   \n",
       "475   none  ...  X1ILHgwtdueiSTYr8XdK        None -0.896 -0.156 -0.292   \n",
       "476   none  ...  HAETRoo2NZj9oKG8JBAB        None   1.06  0.132  0.504   \n",
       "477   none  ...  4GImmlvjC676xXRyFECn        None  0.648  0.008 -0.688   \n",
       "\n",
       "                                             createdAt class1  class2  class3  \\\n",
       "0    {'_seconds': 1603708000, '_nanoseconds': 54000...      0       0       1   \n",
       "1    {'_seconds': 1607352646, '_nanoseconds': 12600...      1       1       1   \n",
       "2    {'_seconds': 1604935813, '_nanoseconds': 52500...      1       0       1   \n",
       "3    {'_seconds': 1604936210, '_nanoseconds': 91100...      0       1       0   \n",
       "4    {'_seconds': 1607771892, '_nanoseconds': 46000...      1       1       0   \n",
       "..                                                 ...    ...     ...     ...   \n",
       "473  {'_seconds': 1636904824, '_nanoseconds': 43000...      0       0       1   \n",
       "474  {'_seconds': 1636898959, '_nanoseconds': 98800...      1       0       0   \n",
       "475  {'_seconds': 1636483781, '_nanoseconds': 19300...      1       0       0   \n",
       "476  {'_seconds': 1620400379, '_nanoseconds': 62000...      0       0       0   \n",
       "477  {'_seconds': 1624128012, '_nanoseconds': 29500...      0       0       0   \n",
       "\n",
       "     occupants  \n",
       "0            1  \n",
       "1            2  \n",
       "2            2  \n",
       "3            1  \n",
       "4            2  \n",
       "..         ...  \n",
       "473          1  \n",
       "474          1  \n",
       "475          1  \n",
       "476          0  \n",
       "477          0  \n",
       "\n",
       "[478 rows x 21 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assign_occupations(df)\n",
    "assign_occupants(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>occupants</th>\n",
       "      <th>class1</th>\n",
       "      <th>class2</th>\n",
       "      <th>class3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>478 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     occupants  class1  class2  class3\n",
       "0            1       0       0       1\n",
       "1            2       1       1       1\n",
       "2            2       1       0       1\n",
       "3            1       0       1       0\n",
       "4            2       1       1       0\n",
       "..         ...     ...     ...     ...\n",
       "473          1       0       0       1\n",
       "474          1       1       0       0\n",
       "475          1       1       0       0\n",
       "476          0       0       0       0\n",
       "477          0       0       0       0\n",
       "\n",
       "[478 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_info = complete_df[['occupants', 'class1', 'class2', 'class3']]\n",
    "df_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class1</th>\n",
       "      <th>class2</th>\n",
       "      <th>class3</th>\n",
       "      <th>occupants</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>478.000000</td>\n",
       "      <td>478.000000</td>\n",
       "      <td>478.000000</td>\n",
       "      <td>478.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.539749</td>\n",
       "      <td>0.265690</td>\n",
       "      <td>0.382845</td>\n",
       "      <td>1.075314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.498940</td>\n",
       "      <td>0.442163</td>\n",
       "      <td>0.486590</td>\n",
       "      <td>0.767922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           class1      class2      class3   occupants\n",
       "count  478.000000  478.000000  478.000000  478.000000\n",
       "mean     0.539749    0.265690    0.382845    1.075314\n",
       "std      0.498940    0.442163    0.486590    0.767922\n",
       "min      0.000000    0.000000    0.000000    0.000000\n",
       "25%      0.000000    0.000000    0.000000    0.000000\n",
       "50%      1.000000    0.000000    0.000000    1.000000\n",
       "75%      1.000000    1.000000    1.000000    2.000000\n",
       "max      1.000000    1.000000    1.000000    2.000000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DESCRIPTION OF THE NEWLY GENERATED DATASET\n",
    "complete_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPLIT THE DATASET IN TRAIN AND TEST\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(complete_df, test_size=0.15, random_state = 42, stratify=complete_df.occupants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "406\n",
      "72\n"
     ]
    }
   ],
   "source": [
    "#LENGHT OF THE TEST DATASET\n",
    "print(len(train_df))\n",
    "print(len(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-105.20099314637616\n",
      "14.199503543044287\n",
      "(406, 53, 86, 1)\n",
      "0.25862068965517243\n",
      "0.4064039408866995\n",
      "0.33497536945812806\n"
     ]
    }
   ],
   "source": [
    "working_df = train_df\n",
    "#TRAINING WILL BE DONE WITH THE FFTDATA COLUMN\n",
    "train_list = working_df.fftData\n",
    "train_list = np.array(train_list)\n",
    "train_x = []\n",
    "\n",
    "\"\"\" Here is performed the frequency selection part of the preprocessing. \n",
    "    Since the fft spectrum is divided in two spectrum of 128 bits each, for performing frequency selection \n",
    "    we need to select from both the first part and the second one.\n",
    "    only one/fraction of the frequencies are selected.\n",
    "\"\"\"\n",
    "\n",
    "#Select only first third of both images\n",
    "fraction = 3 \n",
    "fraction_data = int(round(128/fraction)) #fraction_data=43 in this case\n",
    "\n",
    "for i in range(len(train_list)):\n",
    "    #print(len(train_list[i]))\n",
    "    #print(len(train_list[i][0]))\n",
    "    \n",
    "    a = np.array(train_list[i])[:, 0 : fraction_data]\n",
    "    b = np.array(train_list[i])[:, 128 : 128 + fraction_data]\n",
    "    c = np.concatenate((a, b), axis=1)\n",
    "    train_x.append(c)\n",
    "train_arr = []\n",
    "for x in range(len(train_x)):\n",
    "    train_arr.append(np.array(train_x[x]))\n",
    "train_list = train_arr \n",
    "\n",
    "\"\"\"\n",
    "zscore normalization part of the preprocessing. correcting the dimension of the network.\n",
    "\"\"\"\n",
    "\n",
    "print(np.mean(train_list))\n",
    "print(np.std(train_list))\n",
    "train_list = scipy.stats.zscore(train_list, axis=None)\n",
    "\n",
    "#max = np.max(train_list)\n",
    "#min = np.min(train_list)\n",
    "#train_list = np.array([[[(x - min) / (max - min) for x in y] for y in z] for z in train_list])\n",
    "\n",
    "\n",
    "train_tensor = tf.convert_to_tensor(train_list)\n",
    "\n",
    "#Third dimension value is 1\n",
    "train_tensor = tf.expand_dims(train_tensor, -1)\n",
    "\n",
    "print(train_tensor.shape)\n",
    "\n",
    "\"\"\"\n",
    "assigning label \n",
    "\"\"\"\n",
    "\n",
    "train_label = working_df[\"occupants\"]\n",
    "\n",
    "#PROPORTIONS OF THE DATASET\n",
    "passengers0 = 0\n",
    "passengers1 = 0\n",
    "passengers2 = 0\n",
    "for occupants in working_df[\"occupants\"]:\n",
    "    if occupants == 0:\n",
    "        passengers0+=1\n",
    "    if occupants == 1:\n",
    "        passengers1+=1\n",
    "    if occupants == 2:\n",
    "        passengers2+=1\n",
    "balancing0 = passengers0/len(train_df)\n",
    "balancing1 = passengers1/len(train_df)\n",
    "balancing2 = passengers2/len(train_df)\n",
    "print(balancing0)\n",
    "print(balancing1)\n",
    "print(balancing2)\n",
    "\n",
    "train_label = tf.keras.utils.to_categorical(train_label, 3)\n",
    "\n",
    "\"\"\"Dimensions of the inputs\"\"\"\n",
    "#53*86 images\n",
    "img_h, img_w = 53, fraction_data*2\n",
    "num_classes=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to run inference on a TFLite model\n",
    "def run_tflite_model(tflite_file, inputs, targets, test_image_indices):\n",
    "\n",
    "  # Initialize the interpreter\n",
    "    interpreter = tf.lite.Interpreter(model_path=str(tflite_file))\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    input_details = interpreter.get_input_details()[0]\n",
    "    output_details = interpreter.get_output_details()[0]\n",
    "\n",
    "    predictions = np.zeros((len(test_image_indices),), dtype=int)\n",
    "    for i, test_image_index in enumerate(test_image_indices):\n",
    "        #print(test_image_index)\n",
    "        test_image = inputs[test_image_index]\n",
    "        test_label = targets[test_image_index]\n",
    "\n",
    "        # Check if the input type is quantized, then rescale input data to uint8\n",
    "        #print(input_details['dtype'])\n",
    "        if input_details['dtype'] == np.int8:\n",
    "            #print(\"correct\")\n",
    "            input_scale, input_zero_point = input_details[\"quantization\"]\n",
    "            test_image = test_image / input_scale + input_zero_point\n",
    "\n",
    "        test_image = np.expand_dims(test_image, axis=0).astype(input_details[\"dtype\"])\n",
    "        interpreter.set_tensor(input_details[\"index\"], test_image)\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
    "        #print(output)\n",
    "        predictions[i] = output.argmax()\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(MP_dim2, conv_kernel_w2, conv_kernel_h2, conv_block2, drop_out_rate2, start_f2, regularizer, inputs, targets, dilation_rate, fraction, average_pooling):\n",
    "    # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training model with MP {MP_dim2}, conv kernel{[conv_kernel_w2, conv_kernel_h2]}, {conv_block2} blocks, {start_f2} filters, average pooling layer {average_pooling}, {drop_out_rate2} do...')\n",
    "    experiment = \"3_classes_dw_test_6\" #¡¡¡¡¡CHANGE FOLDER!!!!!!\n",
    "    acc_per_fold = []\n",
    "    loss_per_fold = []\n",
    "    train_acc_per_fold = []\n",
    "    \n",
    "    acc_per_fold_quant = []\n",
    "    train_acc_per_fold_quant = []\n",
    "    \n",
    "    fold_no = 1\n",
    "    Y_pred_list = []\n",
    "    Y_true_list = []\n",
    "    \n",
    "    #--------------------------------STATIC PARAMETERS------------------------------------------\n",
    "    # Define the K-fold Cross Validator\n",
    "    kfold = KFold(n_splits=10, shuffle=True)\n",
    "    #loo = LeaveOneOut()\n",
    "    \n",
    "    \n",
    "    # Optimization params\n",
    "    # -------------------\n",
    "\n",
    "    # Loss\n",
    "    loss = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "    # learning rate\n",
    "    lr = 0.3e-4\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    # -------------------\n",
    "\n",
    "    # Validation metrics\n",
    "    # ------------------\n",
    "\n",
    "    metrics = ['accuracy']\n",
    "\n",
    "    batch_size = 32\n",
    "    \n",
    "    n_epoch = 400\n",
    "    \n",
    "    \n",
    "    #------------------------------------CALLBACKS----------------------------------------\n",
    "    callbacks = []\n",
    "    \n",
    "    # Early Stopping\n",
    "    # --------------\n",
    "    early_stop = False\n",
    "    if early_stop:\n",
    "        es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=100, restore_best_weights=False,)\n",
    "        callbacks.append(es_callback)\n",
    "    \n",
    "    # ----------------------------------CROSSVALIDATION-----------------------------------\n",
    "    for train, test in kfold.split(inputs, targets):\n",
    "      # Define the model architecture\n",
    "        \n",
    "        print(f'{MP_dim2}, {[conv_kernel_w2, conv_kernel_h2]}, {conv_block2}, {start_f2}, {drop_out_rate2}, fold {fold_no}...')\n",
    "        model = tf.keras.Sequential()\n",
    "        \n",
    "        input_shape = [img_h, img_w, 1]\n",
    "\n",
    "        model.add(tf.keras.layers.MaxPool2D(pool_size=(MP_dim2, MP_dim2), input_shape=input_shape))\n",
    "        \n",
    "        n_filters = start_f2\n",
    "        \n",
    "        for i in range(conv_block2):\n",
    "            # Conv block: Conv2D -> Conv2D -> Activation -> Pooling\n",
    "            print(i+1)\n",
    "            model.add(tf.keras.layers.SeparableConv2D(filters=n_filters, \n",
    "                                             kernel_size=(conv_kernel_h2, conv_kernel_w2),\n",
    "                                             strides=(1, 1),\n",
    "                                             depth_multiplier = n_filters*(i+2),\n",
    "                                             dilation_rate = dilation_rate,\n",
    "                                             activation = 'relu'))\n",
    "        \n",
    "        # AVERAGE POOLING\n",
    "        if average_pooling:\n",
    "            #size = math.floor(53/4)\n",
    "            #for i in range (1, conv_block2):\n",
    "                #size = math.floor(size/2)\n",
    "            model.add(tf.keras.layers.AveragePooling2D(pool_size=(22, 39)))\n",
    "\n",
    "        # Classifier\n",
    "        \n",
    "        model.add(tf.keras.layers.Flatten())\n",
    "        model.add(tf.keras.layers.Dropout(drop_out_rate2))\n",
    "        model.add(tf.keras.layers.Dense(units=num_classes, activation='softmax', kernel_regularizer=regularizer))\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(loss=loss,\n",
    "                    optimizer=optimizer,\n",
    "                    metrics=['accuracy'])\n",
    "        # Fit data to model\n",
    "        class_weights={0: balancing0, 1: balancing1, 2: balancing2}\n",
    "        history = model.fit(inputs[(train)], targets[(train)],\n",
    "                    class_weight=class_weights,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=n_epoch,\n",
    "                    validation_data=(inputs[(test)], targets[(test)]),\n",
    "                    callbacks = callbacks,\n",
    "                    verbose=False)\n",
    "\n",
    "        acc_per_fold.append(history.history['val_accuracy'])\n",
    "        loss_per_fold.append(history.history['val_loss'])\n",
    "        train_acc_per_fold.append(history.history['accuracy'])\n",
    "\n",
    "        Y_prediction = model.predict(inputs[test])\n",
    "        Y_pred_list.append(np.argmax(Y_prediction,axis = 1)) \n",
    "        Y_true_list.append(np.argmax( targets[test],axis = 1))\n",
    "        \n",
    "        #-------------------------------SAVE MODEL-----------------------------------------\n",
    "        \n",
    "        MODELS_DIR = f'models/experiment_{experiment}/fraction_{fraction}/n_epoch_{n_epoch}/{MP_dim2}-{conv_kernel_w2}-{conv_kernel_h2}-{conv_block2}-{start_f2}-{drop_out_rate2}-{dilation_rate}--{average_pooling}/'\n",
    "        try:\n",
    "            if not os.path.exists(MODELS_DIR):\n",
    "                os.makedirs(MODELS_DIR)\n",
    "        except e:\n",
    "            if e.errno != errno.EEXIST:\n",
    "                raise   \n",
    "            # time.sleep might help here\n",
    "            pass\n",
    "            \n",
    "        MODEL_TF = MODELS_DIR + f'fold_{fold_no}'\n",
    "        model.save(MODEL_TF)\n",
    "        # Increase fold number\n",
    "        \n",
    "        \n",
    "            #--------------------------QUANTIZE THE MODEL ----------------------------------\n",
    "        MODEL_TFLITE = MODELS_DIR + f'fold_{fold_no}.tflite'\n",
    "\n",
    "        def representative_dataset():\n",
    "            for data in tf.data.Dataset.from_tensor_slices((inputs)).batch(1).take(100):\n",
    "                #print(data)\n",
    "                yield [tf.dtypes.cast(data, tf.float32)]\n",
    "                \n",
    "\n",
    "        converter = tf.lite.TFLiteConverter.from_saved_model(MODEL_TF)\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        # Enforce integer only quantization\n",
    "        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "        converter.inference_input_type = tf.int8\n",
    "        converter.inference_output_type = tf.int8\n",
    "        # Provide a representative dataset to ensure we quantize correctly.\n",
    "        converter.representative_dataset = representative_dataset\n",
    "        model_tflite = converter.convert()\n",
    "\n",
    "        open(MODEL_TFLITE, \"wb\").write(model_tflite)\n",
    "\n",
    "        predictions = run_tflite_model(MODEL_TFLITE, inputs, targets, test)\n",
    "\n",
    "        accuracy = (np.sum(np.argmax( targets[test],axis = 1) == predictions) * 100) / len(test)\n",
    "        \n",
    "        predictions_train = run_tflite_model(MODEL_TFLITE, inputs, targets, train)\n",
    "        accuracy_train = (np.sum(np.argmax( targets[train],axis = 1) == predictions_train) * 100) / len(train)\n",
    "        \n",
    "        acc_per_fold_quant.append(accuracy)\n",
    "        train_acc_per_fold_quant.append(accuracy_train)\n",
    "        print(f\"accuracy: {np.array(acc_per_fold)[:, -1]}\")\n",
    "        print(f\"quantized accuracy: {acc_per_fold_quant}\")\n",
    "        #print(train_acc_per_fold_quant)\"\"\"\n",
    "        fold_no = fold_no + 1\n",
    "    print(model.summary())\n",
    "    #---------------------------SAVE RESULTS TO JSON---------------------------------------\n",
    "    Y_true = []\n",
    "    Y_pred = []\n",
    "    for i in range(len(Y_pred_list)):\n",
    "        Y_true = np.concatenate((Y_true, Y_true_list[i]))\n",
    "        Y_pred = np.concatenate((Y_pred, Y_pred_list[i]))\n",
    "    row = {'MaxPoolDim' : MP_dim2, \n",
    "           'conv_kernel_dim' : [conv_kernel_w2, conv_kernel_h2], \n",
    "           'n_conv' : conv_block2, \n",
    "           'n_filters' : start_f2, \n",
    "           'dropout' : drop_out_rate2, \n",
    "           'n_epochs' : n_epoch,\n",
    "           'dilation_rate' : dilation_rate,\n",
    "           'average_pooling' : average_pooling,\n",
    "           'train_accuracy' : np.mean(train_acc_per_fold, axis=0).tolist(),\n",
    "           'valid_accuracy' : np.mean(acc_per_fold, axis=0).tolist(),\n",
    "           'Y_true' : Y_true.tolist(), \n",
    "           'Y_pred' : Y_pred.tolist(),\n",
    "           'train_accuracy_quant':  np.mean(train_acc_per_fold_quant), \n",
    "           'valid_accuracy_quant':  np.mean(acc_per_fold_quant)\n",
    "          }\n",
    "    JSON_DIR = f'json_child/experiment_{experiment}/fraction_{fraction}/n_epoch_{n_epoch}'\n",
    "    if not os.path.exists(JSON_DIR):\n",
    "        os.makedirs(JSON_DIR)\n",
    "    try:\n",
    "        with open(f'{JSON_DIR}/{MP_dim2}-{conv_kernel_w2}-{conv_kernel_h2}-{conv_block2}-{start_f2}-{drop_out_rate2}-{dilation_rate}--{average_pooling}.json', 'w') as f:\n",
    "            json.dump(row, f)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "    \n",
    "    #--------------------------PLOT ACCURACIES CURVES ------------------------------\n",
    "    plt.clf()\n",
    "    plt.plot(np.mean(train_acc_per_fold, axis=0))\n",
    "    plt.plot(np.mean(acc_per_fold, axis=0))\n",
    "    plt.savefig(f'{JSON_DIR}/{MP_dim2}-{conv_kernel_w2}-{conv_kernel_h2}-{conv_block2}-{start_f2}-{drop_out_rate2}-{dilation_rate}--{average_pooling}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    \n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 will be trained\n"
     ]
    }
   ],
   "source": [
    "# Merge inputs and targets\n",
    "\n",
    "inputs = np.array(train_tensor)\n",
    "targets = train_label\n",
    "\n",
    "#-----------------------------------GRID SEARCH PARAMETERS ---------------------------\n",
    "\n",
    "#first MaxPool dimension, to reduce the input size.\n",
    "MP_dims = [2]\n",
    "\n",
    "#kernel dimension of filters. \n",
    "conv_kernels = [[3,3],[5,5],[7,7]]\n",
    "\n",
    "#number of convolution blocks\n",
    "conv_blocks = [1]\n",
    "\n",
    "#Dropout rate\n",
    "drop_out_rates = [0.3]\n",
    "\n",
    "#number of filters in each convolutional layer\n",
    "start_fs =  [6, 8, 10, 12, 14, 16, 18, 20, 22]\n",
    "\n",
    "#regularizers to use in the network\n",
    "regularizers = [None]\n",
    "\n",
    "#dilation rates of the convolutional layers\n",
    "dilation_rates = [1, 2]\n",
    "\n",
    "#include average pooling layer\n",
    "average_pooling = [False]\n",
    "\n",
    "n_models = len(MP_dims) * len(conv_kernels) * len(conv_blocks) * len(start_fs) * len(drop_out_rates) * len(dilation_rates) * len(average_pooling)\n",
    "\n",
    "print(f\"{n_models} will be trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one(arg):\n",
    "    train_network(arg[0], arg[1], arg[2],arg[3],arg[4], arg[5], arg[6], arg[7], arg[8], arg[9], arg[10], arg[11])\n",
    "\n",
    "def train_all(args):\n",
    "    execution = 0;\n",
    "    for elem in args:\n",
    "        execution +=1\n",
    "        print(f\"Execution nº {execution}\")\n",
    "        train_one(elem)\n",
    "    return 0\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = []\n",
    "    for MP_dim in MP_dims:\n",
    "        for conv_kernel in conv_kernels:\n",
    "            for conv_block in conv_blocks:\n",
    "                for drop_out_rate in drop_out_rates:\n",
    "                    for start_f in start_fs:\n",
    "                        for regularizer in regularizers:\n",
    "                            for dilation_rate in dilation_rates:\n",
    "                                for avg_pooling in average_pooling:\n",
    "                                    args.append([MP_dim, conv_kernel[0], conv_kernel[1], conv_block, drop_out_rate, start_f, regularizer, inputs, targets, dilation_rate, fraction, avg_pooling])\n",
    "\n",
    "    \n",
    "    results = train_all(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train best architecture with the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"retrain the best performing network on train + validation dataset with different seeds\"\"\" \n",
    "\n",
    "accuracies=[]\n",
    "def train_random_states(amount):\n",
    "    for i in range(amount):\n",
    "        print('----------------------------------------------------------------------------')\n",
    "        print(f'RND STATE NUMBER = {i}')\n",
    "        print('----------------------------------------------------------------------------')\n",
    "        \"\"\"SPLIT THE DATASET\"\"\"\n",
    "        from sklearn.model_selection import train_test_split\n",
    "\n",
    "        train_df, test_df = train_test_split(df, test_size=0.15, random_state = i)\n",
    "        \n",
    "        \"\"\"DATA PREPROCESSING\"\"\"\n",
    "        working_df = train_df\n",
    "        #TRAINING WILL BE DONE WITH THE FFTDATA COLUMN\n",
    "        train_list = working_df.fftData\n",
    "        train_list = np.array(train_list)\n",
    "        train_x = []\n",
    "\n",
    "        \"\"\" Here is performed the frequency selection part of the preprocessing. \n",
    "            Since the fft spectrum is divided in two spectrum of 128 bits each, for performing frequency selection \n",
    "            we need to select from both the first part and the second one.\n",
    "            only one/fraction of the frequencies are selected.\n",
    "        \"\"\"\n",
    "\n",
    "        #Select only first third of both images\n",
    "        fraction = 3 \n",
    "        fraction_data = int(round(128/fraction)) #fraction_data=43 in this case\n",
    "\n",
    "        for i in range(len(train_list)):\n",
    "            #print(len(train_list[i]))\n",
    "            #print(len(train_list[i][0]))\n",
    "\n",
    "            a = np.array(train_list[i])[:, 0 : fraction_data]\n",
    "            b = np.array(train_list[i])[:, 128 : 128 + fraction_data]\n",
    "            c = np.concatenate((a, b), axis=1)\n",
    "            train_x.append(c)\n",
    "        train_arr = []\n",
    "        for x in range(len(train_x)):\n",
    "            train_arr.append(np.array(train_x[x]))\n",
    "        train_list = train_arr \n",
    "\n",
    "        \"\"\"\n",
    "        zscore normalization part of the preprocessing. correcting the dimension of the network.\n",
    "        \"\"\"\n",
    "\n",
    "        print(np.mean(train_list))\n",
    "        print(np.std(train_list))\n",
    "        train_list = scipy.stats.zscore(train_list, axis=None)\n",
    "\n",
    "        #max = np.max(train_list)\n",
    "        #min = np.min(train_list)\n",
    "        #train_list = np.array([[[(x - min) / (max - min) for x in y] for y in z] for z in train_list])\n",
    "\n",
    "\n",
    "        train_tensor = tf.convert_to_tensor(train_list)\n",
    "\n",
    "        #Third dimension value is 1\n",
    "        train_tensor = tf.expand_dims(train_tensor, -1)\n",
    "\n",
    "        print(train_tensor.shape)\n",
    "\n",
    "        \"\"\"\n",
    "        assigning label \n",
    "        \"\"\"\n",
    "\n",
    "        train_label = working_df[\"occupants\"]\n",
    "\n",
    "        #PROPORTIONS OF THE DATASET\n",
    "        passengers0 = 0\n",
    "        passengers1 = 0\n",
    "        passengers2 = 0\n",
    "        for occupants in working_df[\"occupants\"]:\n",
    "            if occupants == 0:\n",
    "                passengers0+=1\n",
    "            if occupants == 1:\n",
    "                passengers1+=1\n",
    "            if occupants == 2:\n",
    "                passengers2+=1\n",
    "        balancing0 = passengers0/len(train_df)\n",
    "        balancing1 = passengers1/len(train_df)\n",
    "        balancing2 = passengers2/len(train_df)\n",
    "        print(balancing0)\n",
    "        print(balancing1)\n",
    "        print(balancing2)\n",
    "\n",
    "        train_label = tf.keras.utils.to_categorical(train_label, 3)\n",
    "\n",
    "        \"\"\"Dimensions of the inputs\"\"\"\n",
    "        #53*86 images\n",
    "        img_h, img_w = 53, fraction_data*2\n",
    "        num_classes=3\n",
    "        \n",
    "        \"\"\"\n",
    "        Perform the same preprocessing steps of the training set to the test set too.\n",
    "        \"\"\"\n",
    "        test_labels = np.array(test_df[\"occupants\"])\n",
    "\n",
    "        test_list = test_df[\"fftData\"]\n",
    "        test_list = np.array(test_list)\n",
    "        test_x = []\n",
    "\n",
    "        fraction = 3 \n",
    "        fraction_data = int(round(128/fraction))\n",
    "\n",
    "        for i in range(len(test_list)):\n",
    "\n",
    "            a = np.array(test_list[i])[:, 0 : fraction_data]\n",
    "            b = np.array(test_list[i])[:, 128 : 128 + fraction_data]\n",
    "            c = np.concatenate((a, b), axis=1)\n",
    "            test_x.append(c)\n",
    "        test_arr = []\n",
    "        for x in range(len(test_x)):\n",
    "            test_arr.append(np.array(test_x[x]))\n",
    "        test_list = test_arr \n",
    "\n",
    "\n",
    "        print(np.mean(test_list))\n",
    "        print(np.std(test_list))\n",
    "        test_list = scipy.stats.zscore(test_list, axis=None)\n",
    "\n",
    "        #max = np.max(train_list)\n",
    "        #min = np.min(train_list)\n",
    "        #train_list = np.array([[[(x - min) / (max - min) for x in y] for y in z] for z in train_list])\n",
    "        test_tensor = tf.convert_to_tensor(test_list)\n",
    "        test_tensor = tf.expand_dims(test_tensor, -1)\n",
    "        print(test_tensor.shape)\n",
    "        test_images = test_tensor\n",
    "        \n",
    "        \"\"\"NETWORK DESIGN\"\"\"\n",
    "        # Loss\n",
    "        loss = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "        # learning rate\n",
    "        lr = 0.3e-4\n",
    "\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "        # -------------------\n",
    "\n",
    "        # Validation metrics\n",
    "        # ------------------\n",
    "\n",
    "        metrics = ['accuracy']\n",
    "\n",
    "        batch_size = 32\n",
    "\n",
    "        callbacks = []\n",
    "\n",
    "        n_epoch = 300\n",
    "\n",
    "        conv_kernels = [3]\n",
    "\n",
    "        MP_dims = [2]\n",
    "\n",
    "        #number of convolution blocks\n",
    "        conv_blocks = [1]\n",
    "\n",
    "        #Dropout rate\n",
    "        drop_out_rates = [0.3]\n",
    "\n",
    "        #numbers of starting filters\n",
    "        start_fs = [12]\n",
    "\n",
    "        #dilation_rate\n",
    "        dilation_rates = [2]\n",
    "\n",
    "        regularizer = None\n",
    "\n",
    "        for MP_dim in MP_dims:\n",
    "            for conv_kernel in conv_kernels:\n",
    "                for conv_block in conv_blocks:\n",
    "                    for drop_out_rate in drop_out_rates:\n",
    "                        for n_filter in start_fs:\n",
    "                            for dilation_rate in dilation_rates:\n",
    "                                \n",
    "                                experiment = f\"3_classes_avg_BEST_{i}\"\n",
    "                                \n",
    "                                model = tf.keras.Sequential()\n",
    "\n",
    "                                input_shape = [img_h, img_w, 1]\n",
    "\n",
    "\n",
    "                                model = tf.keras.Sequential()\n",
    "\n",
    "                                model.add(tf.keras.layers.MaxPool2D(pool_size=(MP_dim, MP_dim), input_shape=input_shape))\n",
    "\n",
    "                                for i in range(conv_block):\n",
    "                                    # Conv block: Conv2D -> Conv2D -> Activation -> Pooling\n",
    "                                    model.add(tf.keras.layers.SeparableConv2D(filters=n_filter, \n",
    "                                                     kernel_size=(conv_kernel, conv_kernel),\n",
    "                                                     strides=(1, 1),\n",
    "                                                     depth_multiplier = n_filter*(i+2),\n",
    "                                                     dilation_rate = dilation_rate,\n",
    "                                                     activation = 'relu'))\n",
    "\n",
    "\n",
    "                                # Classifier\n",
    "\n",
    "                                model.add(tf.keras.layers.Flatten())\n",
    "                                model.add(tf.keras.layers.Dropout(drop_out_rate))\n",
    "                                model.add(tf.keras.layers.Dense(units=num_classes, activation='sigmoid', kernel_regularizer=regularizer))\n",
    "\n",
    "                                # Compile the model\n",
    "                                model.compile(loss=loss,\n",
    "                                            optimizer=optimizer,\n",
    "                                            metrics=['accuracy'])\n",
    "\n",
    "\n",
    "                                # Generate a print\n",
    "                                print('------------------------------------------------------------------------')\n",
    "\n",
    "                                # Fit data to model\n",
    "                                class_weights={0: balancing0, 1: balancing1, 2: balancing2}\n",
    "                                history = model.fit(inputs, targets,\n",
    "                                            class_weight=class_weights,\n",
    "                                            batch_size=batch_size,\n",
    "                                            epochs=n_epoch,\n",
    "                                            callbacks = callbacks,\n",
    "                                            verbose=False)\n",
    "\n",
    "                                MODELS_DIR = f'models_full_train/{MP_dim}-{conv_kernel}-{conv_block}-{n_filter}-{drop_out_rate}-{dilation_rate}--{average_pooling}/'\n",
    "                                if not os.path.exists(MODELS_DIR):\n",
    "                                    os.makedirs(MODELS_DIR)\n",
    "                                MODEL_TF = MODELS_DIR + f'model'\n",
    "                                MODEL_TFLITE_MICRO = MODELS_DIR + 'model.cc'\n",
    "                                MODEL_TFLITE = MODELS_DIR + 'model.tflite'\n",
    "                                model.save(MODEL_TF)\n",
    "\n",
    "                                def representative_dataset():\n",
    "                                    for data in tf.data.Dataset.from_tensor_slices((inputs)).batch(1).take(100):\n",
    "                                        #print(data)\n",
    "                                        yield [tf.dtypes.cast(data, tf.float32)]\n",
    "\n",
    "\n",
    "                                converter = tf.lite.TFLiteConverter.from_saved_model(MODEL_TF)\n",
    "                                converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "                                # Enforce integer only quantization\n",
    "                                converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "                                converter.inference_input_type = tf.int8\n",
    "                                converter.inference_output_type = tf.int8\n",
    "                                # Provide a representative dataset to ensure we quantize correctly.\n",
    "                                converter.representative_dataset = representative_dataset\n",
    "                                model_tflite = converter.convert()\n",
    "\n",
    "                                open(MODEL_TFLITE, \"wb\").write(model_tflite)\n",
    "                                \n",
    "        outs = model(test_images)\n",
    "        predicted = np.argmax(outs, axis=1)\n",
    "        print(predicted)\n",
    "        print(test_labels)\n",
    "\n",
    "        from sklearn.metrics import confusion_matrix\n",
    "        conf = confusion_matrix(test_labels, predicted)\n",
    "        accuracy = 0\n",
    "        top = 0\n",
    "        bottom = 0\n",
    "        for j in range(0,len(conf)):\n",
    "            top += conf[j][j]\n",
    "            for k in range(0,len(conf[j])):\n",
    "                bottom += conf[j][k]\n",
    "        accuracy = top/bottom\n",
    "        print(f\"accuracy of experiment_{experiment}_{i} = {accuracy}\")\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "    row = {'accuracy' : accuracies}\n",
    "    JSON_DIR = f'json_child/experiment_3_classes_avg_BEST/fraction_{fraction}/n_epoch_300'\n",
    "    if not os.path.exists(JSON_DIR):\n",
    "        os.makedirs(JSON_DIR)\n",
    "    try:\n",
    "        with open(f'{JSON_DIR}/accuracies.json', 'w') as f:\n",
    "            json.dump(row, f)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_random_states(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results are saved on a .json file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
